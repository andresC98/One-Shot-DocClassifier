{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[TFG Ch5] Document classification using a single article.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfly-WVy8yF4",
        "colab_type": "text"
      },
      "source": [
        "## Installing and importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVytEOrAUjuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "f9391a44-0bc4-4f74-d5f4-b3015f63b8f4"
      },
      "source": [
        "#Prerequisites (external libraries not found in Google Colab)\n",
        "!pip install wikipedia\n",
        "!pip install wikipedia-api\n",
        "###Baseline classifiers libraries###\n",
        "\n",
        "#Keras Neural Network (Feedforward)\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from keras.backend import clear_session\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#Computations \n",
        "import numpy as np\n",
        "import random\n",
        "#Tables\n",
        "import pandas as pd\n",
        "from google.colab.data_table import DataTable\n",
        "#Sklearn classifiers and utils\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.4.5.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=1786e0317d4163ff28b088299f002590172524d5d5fdb1d0477ed93d915e5d00\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting wikipedia-api\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/3d/289963bbf51f8d00cdf7483cdc2baee25ba877e8b4eb72157c47211e3b57/Wikipedia-API-0.5.4.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from wikipedia-api) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->wikipedia-api) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-cp36-none-any.whl size=13462 sha256=b3a51303dfaa73b7579af6ef6728c9f2a4f2932cd8f783a5c57c50af11db43ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/40/42/ba1d497f3712281b659dd65b566fc868035c859239571a725a\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0fhI6M85z9",
        "colab_type": "text"
      },
      "source": [
        "### Project library modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXe918W6Uozt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b9ebb85d-a7f8-4ec6-9a75-629e1794f11a"
      },
      "source": [
        "!git clone https://github.com/andresC98/One-Shot-DocClassifier.git\n",
        "%cd One-Shot-DocClassifier/lib\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'One-Shot-DocClassifier'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (208/208), done.\u001b[K\n",
            "remote: Total 285 (delta 155), reused 183 (delta 75), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (285/285), 1.94 MiB | 7.40 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/One-Shot-DocClassifier/lib\n",
            "arxiv_parser.py  doc_utils.py  max_sim_classifier.py  wiki_parser.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZM4CZ7NUuBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "7afa1ab3-a55b-4d40-f51a-8bc63ed7200c"
      },
      "source": [
        "###Importing project library###\n",
        "import doc_utils #cleaning and other helper functions (visualization...)\n",
        "from max_sim_classifier import MaxSimClassifier #maximum similarity classifier\n",
        "#Parser libraries: wikipedia dataset and arxiv dataset\n",
        "from arxiv_parser import arxiv_parser\n",
        "from wiki_parser import concurrentGetWikiFullPage, concurrentGetAllCatArticles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpWwy0JyCgWR",
        "colab_type": "text"
      },
      "source": [
        "## Data retrieval by Parsers and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ndXzdm-U2pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d27ac6bc-69ec-45bb-c7c1-c21df0645eba"
      },
      "source": [
        "engineering_topic_defs = concurrentGetWikiFullPage(topics_list = doc_utils.ALL_TOPICS )\n",
        "\n",
        "engineering_articles, n_test_samples = concurrentGetAllCatArticles(doc_utils.ALL_TOPICS, full_text_test=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining full wikipedia page for the topic: Chemical engineering. (Definition of Class #[0])Obtaining full wikipedia page for the topic: Biomedical engineering. (Definition of Class #[1])\n",
            "\n",
            "Obtaining full wikipedia page for the topic: Civil engineering. (Definition of Class #[2])\n",
            "Obtaining full wikipedia page for the topic: Electrical engineering. (Definition of Class #[3])Obtaining full wikipedia page for the topic: Mechanical engineering. (Definition of Class #[4])\n",
            "\n",
            "Obtaining full wikipedia page for the topic: Aerospace engineering. (Definition of Class #[5])\n",
            "Obtaining full wikipedia page for the topic: Software engineering. (Definition of Class #[6])\n",
            "Obtaining full wikipedia page for the topic: Industrial engineering. (Definition of Class #[7])\n",
            "Obtaining full wikipedia page for the topic: Computer engineering. (Definition of Class #[8])\n",
            "Retrieved 38 articles from category topic 'Computer engineering'[TopicID:8]\n",
            "Retrieved 57 articles from category topic 'Software engineering'[TopicID:6]\n",
            "Retrieved 67 articles from category topic 'Chemical engineering'[TopicID:0]\n",
            "Retrieved 74 articles from category topic 'Industrial engineering'[TopicID:7]\n",
            "Retrieved 74 articles from category topic 'Biomedical engineering'[TopicID:1]\n",
            "Retrieved 141 articles from category topic 'Electrical engineering'[TopicID:3]\n",
            "Retrieved 153 articles from category topic 'Civil engineering'[TopicID:2]\n",
            "Retrieved 175 articles from category topic 'Aerospace engineering'[TopicID:5]\n",
            "Retrieved 216 articles from category topic 'Mechanical engineering'[TopicID:4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek5HVXsUVTuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "467d2c8b-aac0-42ab-c62e-4e2142923fbf"
      },
      "source": [
        "arxiv_topic_defs = concurrentGetWikiFullPage(topics_list = doc_utils.ARXIV_WIKI_TOPICS )\n",
        "\n",
        "arxiv_dataset, paperslist = arxiv_parser(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining full wikipedia page for the topic: Computer science. (Definition of Class #[0])\n",
            "Obtaining full wikipedia page for the topic: Economics. (Definition of Class #[1])\n",
            "Obtaining full wikipedia page for the topic: Systems engineering. (Definition of Class #[2])\n",
            "Obtaining full wikipedia page for the topic: Mathematics. (Definition of Class #[3])\n",
            "Obtaining full wikipedia page for the topic: Astrophysics. (Definition of Class #[4])\n",
            "Obtaining full wikipedia page for the topic: Computational biology. (Definition of Class #[5])\n",
            "Obtaining full wikipedia page for the topic: Quantitative finance. (Definition of Class #[6])\n",
            "Obtaining full wikipedia page for the topic: Statistics. (Definition of Class #[7])\n",
            "Retrieving papers for subject: computer_science\n",
            "Retrieving papers for subject: economics\n",
            "Retrieving papers for subject: eess\n",
            "Retrieving papers for subject: mathematics\n",
            "Retrieving papers for subject: physics\n",
            "Retrieving papers for subject: q_biology\n",
            "Retrieving papers for subject: q_finance\n",
            "Retrieving papers for subject: statistics\n",
            "Retrieved 800 papers in total from 8 subjects (100 from each).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGjwfwKBcM_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MSC ,SVM and MNB compatible (clf_data): NOTE: Preprocess type selected in pipeline later\n",
        "x_train_CLF_W_defs, y_train_CLF_W_defs, x_test_CLF_W_full, y_test_CLF_W_full = doc_utils.processClassifierData(engineering_topic_defs, engineering_articles,topics = doc_utils.ALL_TOPICS,dataset_type =\"wiki\")\n",
        "x_train_CLF_A_defs, y_train_CLF_A_defs, x_test_CLF_A_full, y_test_CLF_A_full = doc_utils.processClassifierData(arxiv_topic_defs, arxiv_dataset,topics = doc_utils.ARXIV_WIKI_TOPICS,dataset_type =\"arxiv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiO92KSQ9TxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_FFNN_W_C_defs, y_train_FFNN_W_C_defs, x_test_FFNN_W_C_full, y_test_FFNN_W_C_full, dictionary_FFNN_W_C = doc_utils.processNeuralNetData(engineering_topic_defs,engineering_articles,dataset_type = \"wiki\",preprocess = 'custom')\n",
        "x_train_FFNN_A_C_defs, y_train_FFNN_A_C_defs, x_test_FFNN_A_C_full, y_test_FFNN_A_C_full, dictionary_FFNN_A_C = doc_utils.processNeuralNetData(arxiv_topic_defs,arxiv_dataset,dataset_type = \"arxiv\",preprocess = 'custom')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnChq2s5ca-k",
        "colab_type": "text"
      },
      "source": [
        "#Document classification using an article per category - Baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXRSbG21nUJ",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc4nMF8e4D73",
        "colab_type": "text"
      },
      "source": [
        "### Wiki dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVP29qyQ4FZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "14f17fd7-a776-45cc-82ec-636b39ad13d2"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "\n",
        "for i in range(30):\n",
        "    x_eng_articles = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_eng_labels = list()\n",
        "    for category_arts in engineering_articles:\n",
        "        n_articles = len(category_arts[0])\n",
        "        category_class_id = category_arts[1]\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Article #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_eng_articles[category_class_id] = category_arts[0][id]\n",
        "        y_eng_labels.append(category_class_id)\n",
        "\n",
        "\n",
        "    svm = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(1,1))),\n",
        "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                    ('clf', SVC(C=0.01, gamma=0.01)),\n",
        "                    ])\n",
        "    svm.fit(x_eng_articles, y_eng_labels)\n",
        "\n",
        "    y_test_preds_proba = svm.decision_function(x_test_CLF_W_full)\n",
        "    top1_acc, top2_acc = doc_utils.top2_acc(y_test_preds_proba, y_test_CLF_W_full,verbose=0)\n",
        "    \n",
        "    print(\"Run #{}. SVM Top-1 Acc: {:.4f}, Top-2 Acc: {:.4f}\".format(i, top1_acc, top2_acc))\n",
        "    top2_acc_list.append(top2_acc)\n",
        "    top1_acc_list.append(top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. SVM Top-1 Acc: 0.3407, Top-2 Acc: 0.4844\n",
            "Run #1. SVM Top-1 Acc: 0.3045, Top-2 Acc: 0.4533\n",
            "Run #2. SVM Top-1 Acc: 0.2372, Top-2 Acc: 0.4101\n",
            "Run #3. SVM Top-1 Acc: 0.3779, Top-2 Acc: 0.5457\n",
            "Run #4. SVM Top-1 Acc: 0.3508, Top-2 Acc: 0.4814\n",
            "Run #5. SVM Top-1 Acc: 0.3106, Top-2 Acc: 0.4774\n",
            "Run #6. SVM Top-1 Acc: 0.3598, Top-2 Acc: 0.5065\n",
            "Run #7. SVM Top-1 Acc: 0.3930, Top-2 Acc: 0.5859\n",
            "Run #8. SVM Top-1 Acc: 0.3106, Top-2 Acc: 0.4281\n",
            "Run #9. SVM Top-1 Acc: 0.3176, Top-2 Acc: 0.4894\n",
            "Run #10. SVM Top-1 Acc: 0.3106, Top-2 Acc: 0.4663\n",
            "Run #11. SVM Top-1 Acc: 0.3226, Top-2 Acc: 0.4854\n",
            "Run #12. SVM Top-1 Acc: 0.3015, Top-2 Acc: 0.5146\n",
            "Run #13. SVM Top-1 Acc: 0.3417, Top-2 Acc: 0.4985\n",
            "Run #14. SVM Top-1 Acc: 0.3387, Top-2 Acc: 0.4734\n",
            "Run #15. SVM Top-1 Acc: 0.3910, Top-2 Acc: 0.5246\n",
            "Run #16. SVM Top-1 Acc: 0.3698, Top-2 Acc: 0.5749\n",
            "Run #17. SVM Top-1 Acc: 0.3558, Top-2 Acc: 0.5106\n",
            "Run #18. SVM Top-1 Acc: 0.3528, Top-2 Acc: 0.5206\n",
            "Run #19. SVM Top-1 Acc: 0.2754, Top-2 Acc: 0.4955\n",
            "Run #20. SVM Top-1 Acc: 0.2995, Top-2 Acc: 0.4764\n",
            "Run #21. SVM Top-1 Acc: 0.2985, Top-2 Acc: 0.4362\n",
            "Run #22. SVM Top-1 Acc: 0.2693, Top-2 Acc: 0.4000\n",
            "Run #23. SVM Top-1 Acc: 0.3266, Top-2 Acc: 0.4432\n",
            "Run #24. SVM Top-1 Acc: 0.4030, Top-2 Acc: 0.5427\n",
            "Run #25. SVM Top-1 Acc: 0.3286, Top-2 Acc: 0.5427\n",
            "Run #26. SVM Top-1 Acc: 0.3990, Top-2 Acc: 0.5809\n",
            "Run #27. SVM Top-1 Acc: 0.3256, Top-2 Acc: 0.4663\n",
            "Run #28. SVM Top-1 Acc: 0.4623, Top-2 Acc: 0.6271\n",
            "Run #29. SVM Top-1 Acc: 0.3678, Top-2 Acc: 0.5186\n",
            "\n",
            "Avg Top-2 acc:0.4987+-0.0519\n",
            "Avg Top-1 acc:0.3381+-0.0453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkifOeSo4A0J",
        "colab_type": "text"
      },
      "source": [
        "### arXiv dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA-CTwQb0dUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "25016a55-7c35-44a1-a3e2-1d7e749895c6"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "\n",
        "for i in range(30):\n",
        "    x_arxiv_papers = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_arxiv_labels = list()\n",
        "    for category in arxiv_dataset:\n",
        "        n_papers = len(category['papers'])\n",
        "        category_class_id = category['label']\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Paper #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_arxiv_papers[category_class_id] = \". \".join([category['papers'][id]['title'], category['papers'][id]['abstract']])\n",
        "        y_arxiv_labels.append(category_class_id)\n",
        "\n",
        "\n",
        "    svm = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(1,1))),\n",
        "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                    ('clf', SVC(C=0.01, gamma=0.01)),\n",
        "                    ])\n",
        "    svm.fit(x_arxiv_papers, y_arxiv_labels)\n",
        "\n",
        "    y_test_preds_proba = svm.decision_function(x_test_CLF_A_full)\n",
        "    top1_acc, top2_acc = doc_utils.top2_acc(y_test_preds_proba, y_test_CLF_A_full,verbose=0)\n",
        "    \n",
        "    print(\"Run #{}. SVM Top-1 Acc: {:.4f}, Top-2 Acc: {:.4f}\".format(i, top1_acc, top2_acc))\n",
        "    top2_acc_list.append(top2_acc)\n",
        "    top1_acc_list.append(top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. SVM Top-1 Acc: 0.2462, Top-2 Acc: 0.4263\n",
            "Run #1. SVM Top-1 Acc: 0.2025, Top-2 Acc: 0.3625\n",
            "Run #2. SVM Top-1 Acc: 0.2150, Top-2 Acc: 0.3738\n",
            "Run #3. SVM Top-1 Acc: 0.2975, Top-2 Acc: 0.4913\n",
            "Run #4. SVM Top-1 Acc: 0.2900, Top-2 Acc: 0.4275\n",
            "Run #5. SVM Top-1 Acc: 0.2712, Top-2 Acc: 0.4213\n",
            "Run #6. SVM Top-1 Acc: 0.1888, Top-2 Acc: 0.3212\n",
            "Run #7. SVM Top-1 Acc: 0.2475, Top-2 Acc: 0.4025\n",
            "Run #8. SVM Top-1 Acc: 0.2375, Top-2 Acc: 0.4037\n",
            "Run #9. SVM Top-1 Acc: 0.1888, Top-2 Acc: 0.3663\n",
            "Run #10. SVM Top-1 Acc: 0.2213, Top-2 Acc: 0.4062\n",
            "Run #11. SVM Top-1 Acc: 0.3212, Top-2 Acc: 0.4688\n",
            "Run #12. SVM Top-1 Acc: 0.2437, Top-2 Acc: 0.4113\n",
            "Run #13. SVM Top-1 Acc: 0.2013, Top-2 Acc: 0.3650\n",
            "Run #14. SVM Top-1 Acc: 0.2437, Top-2 Acc: 0.4037\n",
            "Run #15. SVM Top-1 Acc: 0.2550, Top-2 Acc: 0.4113\n",
            "Run #16. SVM Top-1 Acc: 0.2150, Top-2 Acc: 0.3625\n",
            "Run #17. SVM Top-1 Acc: 0.2737, Top-2 Acc: 0.4350\n",
            "Run #18. SVM Top-1 Acc: 0.3013, Top-2 Acc: 0.4475\n",
            "Run #19. SVM Top-1 Acc: 0.2988, Top-2 Acc: 0.4500\n",
            "Run #20. SVM Top-1 Acc: 0.2500, Top-2 Acc: 0.3912\n",
            "Run #21. SVM Top-1 Acc: 0.2913, Top-2 Acc: 0.4625\n",
            "Run #22. SVM Top-1 Acc: 0.2525, Top-2 Acc: 0.3987\n",
            "Run #23. SVM Top-1 Acc: 0.2475, Top-2 Acc: 0.3925\n",
            "Run #24. SVM Top-1 Acc: 0.2500, Top-2 Acc: 0.4062\n",
            "Run #25. SVM Top-1 Acc: 0.2425, Top-2 Acc: 0.4325\n",
            "Run #26. SVM Top-1 Acc: 0.2250, Top-2 Acc: 0.4150\n",
            "Run #27. SVM Top-1 Acc: 0.2062, Top-2 Acc: 0.3663\n",
            "Run #28. SVM Top-1 Acc: 0.2562, Top-2 Acc: 0.4050\n",
            "Run #29. SVM Top-1 Acc: 0.2475, Top-2 Acc: 0.3900\n",
            "\n",
            "Avg Top-2 acc:0.4073+-0.0358\n",
            "Avg Top-1 acc:0.2476+-0.0341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3G5Sb63p68",
        "colab_type": "text"
      },
      "source": [
        "Avg Top-2 acc:0.4095+-0.0330\n",
        "Avg Top-1 acc:0.2496+-0.0241"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD2v3vIF1o3j",
        "colab_type": "text"
      },
      "source": [
        "## Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCnTnpwK5Zjd",
        "colab_type": "text"
      },
      "source": [
        "### Wiki dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPgEoeP61qk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "46263753-c189-4384-b98b-244675572b15"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "\n",
        "for i in range(30):\n",
        "    x_eng_articles = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_eng_labels = list()\n",
        "    for category_arts in engineering_articles:\n",
        "        n_articles = len(category_arts[0])\n",
        "        category_class_id = category_arts[1]\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Article #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_eng_articles[category_class_id] = category_arts[0][id]\n",
        "        y_eng_labels.append(category_class_id)\n",
        "\n",
        "\n",
        "    mnb = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
        "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                    ('clf', MultinomialNB(alpha=0.01)),\n",
        "                    ])    \n",
        "    mnb.fit(x_eng_articles, y_eng_labels)\n",
        "\n",
        "    y_test_preds_proba = mnb.predict_proba(x_test_CLF_W_full)\n",
        "    top1_acc, top2_acc = doc_utils.top2_acc(y_test_preds_proba, y_test_CLF_W_full,verbose=0)\n",
        "    \n",
        "    print(\"Run #{}. MNB Top-1 Acc: {:.4f}, Top-2 Acc: {:.4f}\".format(i, top1_acc, top2_acc))\n",
        "    top2_acc_list.append(top2_acc)\n",
        "    top1_acc_list.append(top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. MNB Top-1 Acc: 0.3739, Top-2 Acc: 0.5548\n",
            "Run #1. MNB Top-1 Acc: 0.2663, Top-2 Acc: 0.4181\n",
            "Run #2. MNB Top-1 Acc: 0.3789, Top-2 Acc: 0.5337\n",
            "Run #3. MNB Top-1 Acc: 0.2704, Top-2 Acc: 0.4312\n",
            "Run #4. MNB Top-1 Acc: 0.2985, Top-2 Acc: 0.4975\n",
            "Run #5. MNB Top-1 Acc: 0.2372, Top-2 Acc: 0.3578\n",
            "Run #6. MNB Top-1 Acc: 0.3739, Top-2 Acc: 0.5407\n",
            "Run #7. MNB Top-1 Acc: 0.4392, Top-2 Acc: 0.5799\n",
            "Run #8. MNB Top-1 Acc: 0.3216, Top-2 Acc: 0.5005\n",
            "Run #9. MNB Top-1 Acc: 0.3045, Top-2 Acc: 0.4854\n",
            "Run #10. MNB Top-1 Acc: 0.4040, Top-2 Acc: 0.5859\n",
            "Run #11. MNB Top-1 Acc: 0.2955, Top-2 Acc: 0.4844\n",
            "Run #12. MNB Top-1 Acc: 0.2905, Top-2 Acc: 0.4784\n",
            "Run #13. MNB Top-1 Acc: 0.3116, Top-2 Acc: 0.4583\n",
            "Run #14. MNB Top-1 Acc: 0.3206, Top-2 Acc: 0.5085\n",
            "Run #15. MNB Top-1 Acc: 0.3106, Top-2 Acc: 0.4583\n",
            "Run #16. MNB Top-1 Acc: 0.3829, Top-2 Acc: 0.5779\n",
            "Run #17. MNB Top-1 Acc: 0.3638, Top-2 Acc: 0.5307\n",
            "Run #18. MNB Top-1 Acc: 0.2804, Top-2 Acc: 0.4503\n",
            "Run #19. MNB Top-1 Acc: 0.3789, Top-2 Acc: 0.5467\n",
            "Run #20. MNB Top-1 Acc: 0.3829, Top-2 Acc: 0.5548\n",
            "Run #21. MNB Top-1 Acc: 0.3709, Top-2 Acc: 0.5055\n",
            "Run #22. MNB Top-1 Acc: 0.3789, Top-2 Acc: 0.5337\n",
            "Run #23. MNB Top-1 Acc: 0.2533, Top-2 Acc: 0.4241\n",
            "Run #24. MNB Top-1 Acc: 0.3377, Top-2 Acc: 0.4784\n",
            "Run #25. MNB Top-1 Acc: 0.3256, Top-2 Acc: 0.4794\n",
            "Run #26. MNB Top-1 Acc: 0.3658, Top-2 Acc: 0.5186\n",
            "Run #27. MNB Top-1 Acc: 0.3095, Top-2 Acc: 0.4462\n",
            "Run #28. MNB Top-1 Acc: 0.2754, Top-2 Acc: 0.4181\n",
            "Run #29. MNB Top-1 Acc: 0.3216, Top-2 Acc: 0.4693\n",
            "\n",
            "Avg Top-2 acc:0.4936+-0.0541\n",
            "Avg Top-1 acc:0.3308+-0.0488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7HzfNeR5bsM",
        "colab_type": "text"
      },
      "source": [
        "### arXiv dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAGP6pXb5dyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "1418a837-7914-443a-bde6-db3d82b3927d"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "\n",
        "for i in range(30):\n",
        "    x_arxiv_papers = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_arxiv_labels = list()\n",
        "    for category in arxiv_dataset:\n",
        "        n_papers = len(category['papers'])\n",
        "        category_class_id = category['label']\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Paper #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_arxiv_papers[category_class_id] = \". \".join([category['papers'][id]['title'], category['papers'][id]['abstract']])\n",
        "        y_arxiv_labels.append(category_class_id)\n",
        "\n",
        "\n",
        "    mnb = Pipeline([('vect', CountVectorizer(stop_words='english',ngram_range=(1,2))),\n",
        "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                    ('clf', MultinomialNB(alpha=0.01)),\n",
        "                    ]) \n",
        "    mnb.fit(x_arxiv_papers, y_arxiv_labels)\n",
        "\n",
        "    y_test_preds_proba = mnb.predict_proba(x_test_CLF_A_full)\n",
        "    top1_acc, top2_acc = doc_utils.top2_acc(y_test_preds_proba, y_test_CLF_A_full,verbose=0)\n",
        "    \n",
        "    print(\"Run #{}. MNB Top-1 Acc: {:.4f}, Top-2 Acc: {:.4f}\".format(i, top1_acc, top2_acc))\n",
        "    top2_acc_list.append(top2_acc)\n",
        "    top1_acc_list.append(top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. MNB Top-1 Acc: 0.2375, Top-2 Acc: 0.4213\n",
            "Run #1. MNB Top-1 Acc: 0.2938, Top-2 Acc: 0.4675\n",
            "Run #2. MNB Top-1 Acc: 0.2975, Top-2 Acc: 0.4650\n",
            "Run #3. MNB Top-1 Acc: 0.2625, Top-2 Acc: 0.4075\n",
            "Run #4. MNB Top-1 Acc: 0.2375, Top-2 Acc: 0.3775\n",
            "Run #5. MNB Top-1 Acc: 0.2550, Top-2 Acc: 0.4150\n",
            "Run #6. MNB Top-1 Acc: 0.2300, Top-2 Acc: 0.3812\n",
            "Run #7. MNB Top-1 Acc: 0.2600, Top-2 Acc: 0.4437\n",
            "Run #8. MNB Top-1 Acc: 0.2637, Top-2 Acc: 0.4025\n",
            "Run #9. MNB Top-1 Acc: 0.2737, Top-2 Acc: 0.4250\n",
            "Run #10. MNB Top-1 Acc: 0.2587, Top-2 Acc: 0.4138\n",
            "Run #11. MNB Top-1 Acc: 0.2863, Top-2 Acc: 0.4313\n",
            "Run #12. MNB Top-1 Acc: 0.2675, Top-2 Acc: 0.4363\n",
            "Run #13. MNB Top-1 Acc: 0.2537, Top-2 Acc: 0.4100\n",
            "Run #14. MNB Top-1 Acc: 0.2988, Top-2 Acc: 0.4713\n",
            "Run #15. MNB Top-1 Acc: 0.1963, Top-2 Acc: 0.3475\n",
            "Run #16. MNB Top-1 Acc: 0.2288, Top-2 Acc: 0.3688\n",
            "Run #17. MNB Top-1 Acc: 0.2800, Top-2 Acc: 0.4600\n",
            "Run #18. MNB Top-1 Acc: 0.2100, Top-2 Acc: 0.3475\n",
            "Run #19. MNB Top-1 Acc: 0.2712, Top-2 Acc: 0.4537\n",
            "Run #20. MNB Top-1 Acc: 0.2812, Top-2 Acc: 0.4537\n",
            "Run #21. MNB Top-1 Acc: 0.2225, Top-2 Acc: 0.3875\n",
            "Run #22. MNB Top-1 Acc: 0.2400, Top-2 Acc: 0.3975\n",
            "Run #23. MNB Top-1 Acc: 0.2213, Top-2 Acc: 0.3950\n",
            "Run #24. MNB Top-1 Acc: 0.2325, Top-2 Acc: 0.3638\n",
            "Run #25. MNB Top-1 Acc: 0.2737, Top-2 Acc: 0.4300\n",
            "Run #26. MNB Top-1 Acc: 0.2787, Top-2 Acc: 0.4525\n",
            "Run #27. MNB Top-1 Acc: 0.2150, Top-2 Acc: 0.4062\n",
            "Run #28. MNB Top-1 Acc: 0.2137, Top-2 Acc: 0.3538\n",
            "Run #29. MNB Top-1 Acc: 0.2725, Top-2 Acc: 0.4238\n",
            "\n",
            "Avg Top-2 acc:0.4137+-0.0359\n",
            "Avg Top-1 acc:0.2538+-0.0278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdG3-9o51q-j",
        "colab_type": "text"
      },
      "source": [
        "## Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQDfXaB1siV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "#Defining TOP-2 accuracy \n",
        "import functools\n",
        "top2_accuracy = functools.partial(top_k_categorical_accuracy, k=2)\n",
        "top2_accuracy.__name__ = 'TOP2_ACC'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHpluskXLHDZ",
        "colab_type": "text"
      },
      "source": [
        "### Wiki dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORU_Widwmo8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "0705ccfd-0832-4533-c542-89a552f1719c"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "cummulative_id = 0\n",
        "for i in range(30):\n",
        "    x_eng_articles = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_eng_labels = list()\n",
        "    cummulative_id = 0\n",
        "\n",
        "    for category_arts in engineering_articles:\n",
        "        n_articles = len(category_arts[0])\n",
        "        category_class_id = category_arts[1]\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Article #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_eng_articles[category_class_id] = x_test_FFNN_W_C_full[cummulative_id + id]\n",
        "        y_eng_labels.append(y_test_FFNN_W_C_full[cummulative_id + id])\n",
        "        cummulative_id += n_articles\n",
        "\n",
        "    x_eng_articles = np.stack( x_eng_articles )\n",
        "    y_eng_labels = np.stack(y_eng_labels)\n",
        "    #y_eng_labels = to_categorical(y_eng_labels)\n",
        "\n",
        "    clear_session()\n",
        "    \n",
        "    ffnn_model = Sequential()\n",
        "    ffnn_model.add(Dense(64, activation='relu', input_shape=(len(dictionary_FFNN_W_C),)))\n",
        "    ffnn_model.add(Dense(len(doc_utils.ALL_TOPICS), activation='softmax'))\n",
        "    ffnn_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy', top2_accuracy])\n",
        "    \n",
        "    hist = ffnn_model.fit(x_eng_articles, y_eng_labels, epochs=1,verbose=0)\n",
        "    \n",
        "    model_top1_acc = ffnn_model.evaluate(x_test_FFNN_W_C_full, y_test_FFNN_W_C_full, verbose=0)[1]\n",
        "    model_top2_acc = ffnn_model.evaluate(x_test_FFNN_W_C_full, y_test_FFNN_W_C_full, verbose=0)[2]\n",
        "    \n",
        "    print(\"Run #{}. FFNN Top-1 Acc: {:.7f}, Top-2 Acc: {:.7f}\".format(i, model_top1_acc, model_top2_acc))\n",
        "    top2_acc_list.append(model_top2_acc)\n",
        "    top1_acc_list.append(model_top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.7f}+-{:.7f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.7f}+-{:.7f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. FFNN Top-1 Acc: 0.2512563, Top-2 Acc: 0.4130653\n",
            "Run #1. FFNN Top-1 Acc: 0.1095477, Top-2 Acc: 0.3135678\n",
            "Run #2. FFNN Top-1 Acc: 0.0984925, Top-2 Acc: 0.2000000\n",
            "Run #3. FFNN Top-1 Acc: 0.1326633, Top-2 Acc: 0.2683417\n",
            "Run #4. FFNN Top-1 Acc: 0.2201005, Top-2 Acc: 0.3115578\n",
            "Run #5. FFNN Top-1 Acc: 0.1557789, Top-2 Acc: 0.2231156\n",
            "Run #6. FFNN Top-1 Acc: 0.1537688, Top-2 Acc: 0.2080402\n",
            "Run #7. FFNN Top-1 Acc: 0.1638191, Top-2 Acc: 0.2984925\n",
            "Run #8. FFNN Top-1 Acc: 0.1698492, Top-2 Acc: 0.2964824\n",
            "Run #9. FFNN Top-1 Acc: 0.2100503, Top-2 Acc: 0.3025126\n",
            "Run #10. FFNN Top-1 Acc: 0.1236181, Top-2 Acc: 0.2301508\n",
            "Run #11. FFNN Top-1 Acc: 0.1427136, Top-2 Acc: 0.2422111\n",
            "Run #12. FFNN Top-1 Acc: 0.2241206, Top-2 Acc: 0.3276382\n",
            "Run #13. FFNN Top-1 Acc: 0.2030151, Top-2 Acc: 0.3015075\n",
            "Run #14. FFNN Top-1 Acc: 0.2261306, Top-2 Acc: 0.3356784\n",
            "Run #15. FFNN Top-1 Acc: 0.1085427, Top-2 Acc: 0.2472362\n",
            "Run #16. FFNN Top-1 Acc: 0.0703518, Top-2 Acc: 0.1497487\n",
            "Run #17. FFNN Top-1 Acc: 0.1396985, Top-2 Acc: 0.2592965\n",
            "Run #18. FFNN Top-1 Acc: 0.1587940, Top-2 Acc: 0.2914573\n",
            "Run #19. FFNN Top-1 Acc: 0.0994975, Top-2 Acc: 0.1738693\n",
            "Run #20. FFNN Top-1 Acc: 0.1989950, Top-2 Acc: 0.3095478\n",
            "Run #21. FFNN Top-1 Acc: 0.1547739, Top-2 Acc: 0.2381909\n",
            "Run #22. FFNN Top-1 Acc: 0.2391960, Top-2 Acc: 0.3507538\n",
            "Run #23. FFNN Top-1 Acc: 0.1738693, Top-2 Acc: 0.3095478\n",
            "Run #24. FFNN Top-1 Acc: 0.0924623, Top-2 Acc: 0.1728643\n",
            "Run #25. FFNN Top-1 Acc: 0.0954774, Top-2 Acc: 0.2130653\n",
            "Run #26. FFNN Top-1 Acc: 0.1798995, Top-2 Acc: 0.2713568\n",
            "Run #27. FFNN Top-1 Acc: 0.1748744, Top-2 Acc: 0.2944724\n",
            "Run #28. FFNN Top-1 Acc: 0.2180905, Top-2 Acc: 0.3467337\n",
            "Run #29. FFNN Top-1 Acc: 0.1155779, Top-2 Acc: 0.2371859\n",
            "\n",
            "Avg Top-2 acc:0.2712563+-0.0589470\n",
            "Avg Top-1 acc:0.1601675+-0.0486848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT-buLtE9CwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "4b4218d2-4932-41c2-fcf1-a796db1833f7"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "cummulative_id = 0\n",
        "for i in range(30):\n",
        "    x_eng_articles = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_eng_labels = list()\n",
        "    cummulative_id = 0\n",
        "\n",
        "    for category_arts in engineering_articles:\n",
        "        n_articles = len(category_arts[0])\n",
        "        category_class_id = category_arts[1]\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Article #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_eng_articles[category_class_id] = x_test_FFNN_W_C_full[cummulative_id + id]\n",
        "        y_eng_labels.append(y_test_FFNN_W_C_full[cummulative_id + id])\n",
        "        cummulative_id += n_articles\n",
        "\n",
        "    x_eng_articles = np.stack( x_eng_articles )\n",
        "    y_eng_labels = np.stack(y_eng_labels)\n",
        "    #y_eng_labels = to_categorical(y_eng_labels)\n",
        "\n",
        "    clear_session()\n",
        "    \n",
        "    ffnn_model = Sequential()\n",
        "    ffnn_model.add(Dense(256, activation='relu', input_shape=(len(dictionary_FFNN_W_C),)))\n",
        "    ffnn_model.add(Dense(len(doc_utils.ALL_TOPICS), activation='softmax'))\n",
        "    ffnn_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy', top2_accuracy])\n",
        "    \n",
        "    hist = ffnn_model.fit(x_eng_articles, y_eng_labels, epochs=5,verbose=0)\n",
        "    \n",
        "    model_top1_acc = ffnn_model.evaluate(x_test_FFNN_W_C_full, y_test_FFNN_W_C_full, verbose=0)[1]\n",
        "    model_top2_acc = ffnn_model.evaluate(x_test_FFNN_W_C_full, y_test_FFNN_W_C_full, verbose=0)[2]\n",
        "    \n",
        "    print(\"Run #{}. FFNN Top-1 Acc: {:.7f}, Top-2 Acc: {:.7f}\".format(i, model_top1_acc, model_top2_acc))\n",
        "    top2_acc_list.append(model_top2_acc)\n",
        "    top1_acc_list.append(model_top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.7f}+-{:.7f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.7f}+-{:.7f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. FFNN Top-1 Acc: 0.2502513, Top-2 Acc: 0.4412060\n",
            "Run #1. FFNN Top-1 Acc: 0.2442211, Top-2 Acc: 0.3829146\n",
            "Run #2. FFNN Top-1 Acc: 0.2211055, Top-2 Acc: 0.3396985\n",
            "Run #3. FFNN Top-1 Acc: 0.1135678, Top-2 Acc: 0.3336684\n",
            "Run #4. FFNN Top-1 Acc: 0.2291457, Top-2 Acc: 0.3889447\n",
            "Run #5. FFNN Top-1 Acc: 0.2814070, Top-2 Acc: 0.3939699\n",
            "Run #6. FFNN Top-1 Acc: 0.2050251, Top-2 Acc: 0.3527638\n",
            "Run #7. FFNN Top-1 Acc: 0.1266332, Top-2 Acc: 0.2914573\n",
            "Run #8. FFNN Top-1 Acc: 0.2562814, Top-2 Acc: 0.4251256\n",
            "Run #9. FFNN Top-1 Acc: 0.1758794, Top-2 Acc: 0.2944724\n",
            "Run #10. FFNN Top-1 Acc: 0.2884422, Top-2 Acc: 0.4140703\n",
            "Run #11. FFNN Top-1 Acc: 0.0613065, Top-2 Acc: 0.2251256\n",
            "Run #12. FFNN Top-1 Acc: 0.2130653, Top-2 Acc: 0.2974874\n",
            "Run #13. FFNN Top-1 Acc: 0.1427136, Top-2 Acc: 0.2683417\n",
            "Run #14. FFNN Top-1 Acc: 0.2030151, Top-2 Acc: 0.3055276\n",
            "Run #15. FFNN Top-1 Acc: 0.1628141, Top-2 Acc: 0.2874372\n",
            "Run #16. FFNN Top-1 Acc: 0.2120603, Top-2 Acc: 0.3949749\n",
            "Run #17. FFNN Top-1 Acc: 0.1889447, Top-2 Acc: 0.3396985\n",
            "Run #18. FFNN Top-1 Acc: 0.1236181, Top-2 Acc: 0.3447236\n",
            "Run #19. FFNN Top-1 Acc: 0.2060301, Top-2 Acc: 0.3115578\n",
            "Run #20. FFNN Top-1 Acc: 0.1417085, Top-2 Acc: 0.2693467\n",
            "Run #21. FFNN Top-1 Acc: 0.2050251, Top-2 Acc: 0.3226131\n",
            "Run #22. FFNN Top-1 Acc: 0.1658292, Top-2 Acc: 0.3236181\n",
            "Run #23. FFNN Top-1 Acc: 0.2412060, Top-2 Acc: 0.3758794\n",
            "Run #24. FFNN Top-1 Acc: 0.2693467, Top-2 Acc: 0.3547739\n",
            "Run #25. FFNN Top-1 Acc: 0.3015075, Top-2 Acc: 0.4693467\n",
            "Run #26. FFNN Top-1 Acc: 0.2824121, Top-2 Acc: 0.4261307\n",
            "Run #27. FFNN Top-1 Acc: 0.1497487, Top-2 Acc: 0.3216080\n",
            "Run #28. FFNN Top-1 Acc: 0.1497487, Top-2 Acc: 0.2733668\n",
            "Run #29. FFNN Top-1 Acc: 0.2271357, Top-2 Acc: 0.3557789\n",
            "\n",
            "Avg Top-2 acc:0.3441876+-0.0572769\n",
            "Avg Top-1 acc:0.2013065+-0.0580229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXqgEf8xLKg8",
        "colab_type": "text"
      },
      "source": [
        "### arXiv dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWgBOFSzElqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "14a86378-d864-4c5a-8dc2-5bb67f57e9bc"
      },
      "source": [
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "for i in range(30):\n",
        "    cummulative_id = 0\n",
        "    x_arxiv_papers = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_arxiv_labels = list()\n",
        "    for category in arxiv_dataset:\n",
        "        n_papers = len(category['papers'])\n",
        "        category_class_id = category['label']\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Article #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_arxiv_papers[category_class_id] = x_test_FFNN_A_C_full[cummulative_id + id]\n",
        "        y_arxiv_labels.append(y_test_FFNN_A_C_full[cummulative_id + id])\n",
        "        cummulative_id += n_articles\n",
        "\n",
        "    x_arxiv_papers = np.stack( x_arxiv_papers )\n",
        "    y_arxiv_labels = np.stack(y_arxiv_labels)\n",
        "\n",
        "    clear_session()\n",
        "    \n",
        "    ffnn_model = Sequential()\n",
        "    ffnn_model.add(Dense(256, activation='relu', input_shape=(len(dictionary_FFNN_A_C),)))\n",
        "    ffnn_model.add(Dense(len(doc_utils.ARXIV_WIKI_TOPICS), activation='softmax'))\n",
        "    ffnn_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy', top2_accuracy])\n",
        "    \n",
        "    hist = ffnn_model.fit(x_arxiv_papers, y_arxiv_labels, epochs=5,verbose=0)\n",
        "    \n",
        "    model_top1_acc = ffnn_model.evaluate(x_test_FFNN_A_C_full, y_test_FFNN_A_C_full, verbose=0)[1]\n",
        "    model_top2_acc = ffnn_model.evaluate(x_test_FFNN_A_C_full, y_test_FFNN_A_C_full, verbose=0)[2]\n",
        "    \n",
        "    print(\"Run #{}. FFNN Top-1 Acc: {:.7f}, Top-2 Acc: {:.7f}\".format(i, model_top1_acc, model_top2_acc))\n",
        "    top2_acc_list.append(model_top2_acc)\n",
        "    top1_acc_list.append(model_top1_acc)\n",
        "\n",
        "print(\"\\nAvg Top-2 acc:{:.7f}+-{:.7f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.7f}+-{:.7f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run #0. FFNN Top-1 Acc: 0.1437500, Top-2 Acc: 0.2875000\n",
            "Run #1. FFNN Top-1 Acc: 0.1575000, Top-2 Acc: 0.2712500\n",
            "Run #2. FFNN Top-1 Acc: 0.1400000, Top-2 Acc: 0.2562500\n",
            "Run #3. FFNN Top-1 Acc: 0.1662500, Top-2 Acc: 0.2900000\n",
            "Run #4. FFNN Top-1 Acc: 0.1750000, Top-2 Acc: 0.2900000\n",
            "Run #5. FFNN Top-1 Acc: 0.1687500, Top-2 Acc: 0.2825000\n",
            "Run #6. FFNN Top-1 Acc: 0.1825000, Top-2 Acc: 0.3087500\n",
            "Run #7. FFNN Top-1 Acc: 0.1712500, Top-2 Acc: 0.2862500\n",
            "Run #8. FFNN Top-1 Acc: 0.1487500, Top-2 Acc: 0.2800000\n",
            "Run #9. FFNN Top-1 Acc: 0.1812500, Top-2 Acc: 0.2700000\n",
            "Run #10. FFNN Top-1 Acc: 0.1587500, Top-2 Acc: 0.2825000\n",
            "Run #11. FFNN Top-1 Acc: 0.1600000, Top-2 Acc: 0.3025000\n",
            "Run #12. FFNN Top-1 Acc: 0.1837500, Top-2 Acc: 0.2675000\n",
            "Run #13. FFNN Top-1 Acc: 0.1700000, Top-2 Acc: 0.2662500\n",
            "Run #14. FFNN Top-1 Acc: 0.1837500, Top-2 Acc: 0.2712500\n",
            "Run #15. FFNN Top-1 Acc: 0.1737500, Top-2 Acc: 0.2987500\n",
            "Run #16. FFNN Top-1 Acc: 0.1775000, Top-2 Acc: 0.2712500\n",
            "Run #17. FFNN Top-1 Acc: 0.1825000, Top-2 Acc: 0.2675000\n",
            "Run #18. FFNN Top-1 Acc: 0.1762500, Top-2 Acc: 0.2937500\n",
            "Run #19. FFNN Top-1 Acc: 0.1712500, Top-2 Acc: 0.2937500\n",
            "Run #20. FFNN Top-1 Acc: 0.1562500, Top-2 Acc: 0.2612500\n",
            "Run #21. FFNN Top-1 Acc: 0.1487500, Top-2 Acc: 0.2762500\n",
            "Run #22. FFNN Top-1 Acc: 0.1687500, Top-2 Acc: 0.2975000\n",
            "Run #23. FFNN Top-1 Acc: 0.1612500, Top-2 Acc: 0.2575000\n",
            "Run #24. FFNN Top-1 Acc: 0.1650000, Top-2 Acc: 0.2987500\n",
            "Run #25. FFNN Top-1 Acc: 0.1612500, Top-2 Acc: 0.2712500\n",
            "Run #26. FFNN Top-1 Acc: 0.1725000, Top-2 Acc: 0.2950000\n",
            "Run #27. FFNN Top-1 Acc: 0.1837500, Top-2 Acc: 0.2675000\n",
            "Run #28. FFNN Top-1 Acc: 0.1662500, Top-2 Acc: 0.2750000\n",
            "Run #29. FFNN Top-1 Acc: 0.1825000, Top-2 Acc: 0.3062500\n",
            "\n",
            "Avg Top-2 acc:0.2814583+-0.0146098\n",
            "Avg Top-1 acc:0.1679583+-0.0121856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIZSXBWDcv3k",
        "colab_type": "text"
      },
      "source": [
        "#Document classification using an article per category - MSC with document embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AtYIFU_yejq",
        "colab_type": "text"
      },
      "source": [
        "## Maximum Similarity Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8vaVQADyiRD",
        "colab_type": "text"
      },
      "source": [
        "### Wiki dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHFXWFlNdUSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "02497ace-c1c6-46ad-a429-c5c6e91bf631"
      },
      "source": [
        "#Accounting for stochasticity of the method\n",
        "best_hparams = {'preprocess': 'custom', 'dm': 0, 'epochs': 50, 'vector_size': 50, 'min_count': 1, 'window': 3}\n",
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "\n",
        "for i in range(30):\n",
        "    x_eng_articles = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_eng_labels = list()\n",
        "    for category_arts in engineering_articles:\n",
        "        n_articles = len(category_arts[0])\n",
        "        category_class_id = category_arts[1]\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Article #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_eng_articles[category_class_id] = category_arts[0][id]\n",
        "        y_eng_labels.append(category_class_id)\n",
        "    \n",
        "    \n",
        "    max_sim_model = MaxSimClassifier(\"wiki\", best_hparams['preprocess'], best_hparams['vector_size'],best_hparams['min_count'],\n",
        "                                    best_hparams['epochs'],best_hparams['dm'],best_hparams['window'],workers=8)\n",
        "\n",
        "    max_sim_model.fit_articles(x_eng_articles, y_eng_labels)\n",
        "\n",
        "    top2_acc = max_sim_model.score(x_test_CLF_W_full, y_test_CLF_W_full,eval=\"top2\") \n",
        "    top1_acc = max_sim_model.score(x_test_CLF_W_full, y_test_CLF_W_full,eval=\"top1\")\n",
        "\n",
        "    print(\"Run {}. MSC Top-1 Acc: {:.4f}, Top-2 Acc.:{:.4f}\".format(i, top1_acc, top2_acc))\n",
        "    top2_acc_list.append(top2_acc)\n",
        "    top1_acc_list.append(top1_acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run 0. MSC Top-1 Acc: 0.3226, Top-2 Acc.:0.5196\n",
            "Run 1. MSC Top-1 Acc: 0.3085, Top-2 Acc.:0.4834\n",
            "Run 2. MSC Top-1 Acc: 0.3568, Top-2 Acc.:0.5166\n",
            "Run 3. MSC Top-1 Acc: 0.1889, Top-2 Acc.:0.3397\n",
            "Run 4. MSC Top-1 Acc: 0.3176, Top-2 Acc.:0.5035\n",
            "Run 5. MSC Top-1 Acc: 0.3216, Top-2 Acc.:0.4764\n",
            "Run 6. MSC Top-1 Acc: 0.3206, Top-2 Acc.:0.4372\n",
            "Run 7. MSC Top-1 Acc: 0.3849, Top-2 Acc.:0.5457\n",
            "Run 8. MSC Top-1 Acc: 0.3085, Top-2 Acc.:0.4563\n",
            "Run 9. MSC Top-1 Acc: 0.2523, Top-2 Acc.:0.4000\n",
            "Run 10. MSC Top-1 Acc: 0.2874, Top-2 Acc.:0.4482\n",
            "Run 11. MSC Top-1 Acc: 0.3578, Top-2 Acc.:0.5367\n",
            "Run 12. MSC Top-1 Acc: 0.2754, Top-2 Acc.:0.4030\n",
            "Run 13. MSC Top-1 Acc: 0.2724, Top-2 Acc.:0.4221\n",
            "Run 14. MSC Top-1 Acc: 0.3236, Top-2 Acc.:0.4945\n",
            "Run 15. MSC Top-1 Acc: 0.3246, Top-2 Acc.:0.4864\n",
            "Run 16. MSC Top-1 Acc: 0.3106, Top-2 Acc.:0.4653\n",
            "Run 17. MSC Top-1 Acc: 0.2201, Top-2 Acc.:0.3477\n",
            "Run 18. MSC Top-1 Acc: 0.3126, Top-2 Acc.:0.4573\n",
            "Run 19. MSC Top-1 Acc: 0.3889, Top-2 Acc.:0.5538\n",
            "Run 20. MSC Top-1 Acc: 0.3508, Top-2 Acc.:0.5236\n",
            "Run 21. MSC Top-1 Acc: 0.2442, Top-2 Acc.:0.3879\n",
            "Run 22. MSC Top-1 Acc: 0.4020, Top-2 Acc.:0.5859\n",
            "Run 23. MSC Top-1 Acc: 0.3377, Top-2 Acc.:0.5075\n",
            "Run 24. MSC Top-1 Acc: 0.3116, Top-2 Acc.:0.4764\n",
            "Run 25. MSC Top-1 Acc: 0.3879, Top-2 Acc.:0.5508\n",
            "Run 26. MSC Top-1 Acc: 0.2714, Top-2 Acc.:0.4412\n",
            "Run 27. MSC Top-1 Acc: 0.4010, Top-2 Acc.:0.5769\n",
            "Run 28. MSC Top-1 Acc: 0.3156, Top-2 Acc.:0.4593\n",
            "Run 29. MSC Top-1 Acc: 0.3407, Top-2 Acc.:0.5286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Bu8xmCeoD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b251c218-6084-40e9-e49b-870cc8017537"
      },
      "source": [
        "print(\"Avg Top-2 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg Top-2 acc:0.4777+-0.0618\n",
            "Avg Top-1 acc:0.3173+-0.0506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jERpy1fd1UXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "3efb6d7c-7a6e-4a6f-d948-b4f165b94be0"
      },
      "source": [
        "x_eng_articles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Deal–Grove model mathematically describes the growth of an oxide layer on the surface of a material.  In particular, it is used to predict and interpret thermal oxidation of silicon in semiconductor device fabrication. The model was first published in 1965 by Bruce Deal and Andrew Grove of Fairchild Semiconductor, building on Mohamed M. Atalla\\'s work on silicon surface passivation by thermal oxidation at Bell Labs in the late 1950s. This served as a step in the development of CMOS devices and the fabrication of integrated circuits.\\n\\nPhysical assumptions\\nThe model assumes that oxidation reaction occurs at the interface between the oxide layer and the substrate material, rather than between the oxide and the ambient gas.  Thus, it considers three phenomena that the oxidizing species undergoes, in this order:\\n\\nIt diffuses from the bulk of the ambient gas to the surface.\\nIt diffuses through the existing oxide layer to the oxide-substrate interface.\\nIt reacts with the substrate.The model assumes that each of these stages proceeds at a rate proportional to the oxidant\\'s concentration.  In the first case, this means Henry\\'s law; in the second, Fick\\'s law of diffusion; in the third, a first-order reaction with respect to the oxidant.  It also assumes steady state conditions, i.e. that transient effects do not appear.\\n\\nResults\\nGiven these assumptions, the flux of oxidant through each of the three phases can be expressed in terms of concentrations, material properties, and temperature.\\n\\n  \\n    \\n      \\n        \\n          J\\n          \\n            g\\n            a\\n            s\\n          \\n        \\n        =\\n        \\n          h\\n          \\n            g\\n          \\n        \\n        (\\n        \\n          C\\n          \\n            g\\n          \\n        \\n        −\\n        \\n          C\\n          \\n            s\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle J_{gas}=h_{g}(C_{g}-C_{s})}\\n  \\n\\n  \\n    \\n      \\n        \\n          J\\n          \\n            o\\n            x\\n            i\\n            d\\n            e\\n          \\n        \\n        =\\n        \\n          D\\n          \\n            o\\n            x\\n          \\n        \\n        \\n          \\n            \\n              \\n                C\\n                \\n                  s\\n                \\n              \\n              −\\n              \\n                C\\n                \\n                  i\\n                \\n              \\n            \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{oxide}=D_{ox}{\\\\frac {C_{s}-C_{i}}{x}}}\\n  \\n\\n  \\n    \\n      \\n        \\n          J\\n          \\n            r\\n            e\\n            a\\n            c\\n            t\\n            i\\n            n\\n            g\\n          \\n        \\n        =\\n        \\n          k\\n          \\n            i\\n          \\n        \\n        \\n          C\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{reacting}=k_{i}C_{i}}\\n  By setting the three fluxes equal to each other \\n  \\n    \\n      \\n        \\n          J\\n          \\n            g\\n            a\\n            s\\n          \\n        \\n        =\\n        \\n          J\\n          \\n            o\\n            x\\n            i\\n            d\\n            e\\n          \\n        \\n        =\\n        \\n          J\\n          \\n            r\\n            e\\n            a\\n            c\\n            t\\n            i\\n            n\\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{gas}=J_{oxide}=J_{reacting}}\\n  , the following relations can be derived: \\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              C\\n              \\n                i\\n              \\n            \\n            \\n              C\\n              \\n                g\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              1\\n              +\\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                /\\n              \\n              \\n                h\\n                \\n                  g\\n                \\n              \\n              +\\n              \\n                \\n                  k\\n                  \\n                    i\\n                  \\n                \\n                x\\n              \\n              \\n                /\\n              \\n              \\n                D\\n                \\n                  o\\n                  x\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {C_{i}}{C_{g}}}={\\\\frac {1}{1+k_{i}/h_{g}+{k_{i}x}/D_{ox}}}}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              C\\n              \\n                s\\n              \\n            \\n            \\n              C\\n              \\n                g\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              1\\n              +\\n              \\n                \\n                  k\\n                  \\n                    i\\n                  \\n                \\n                x\\n              \\n              \\n                /\\n              \\n              \\n                D\\n                \\n                  o\\n                  x\\n                \\n              \\n            \\n            \\n              1\\n              +\\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                /\\n              \\n              \\n                h\\n                \\n                  g\\n                \\n              \\n              +\\n              \\n                \\n                  k\\n                  \\n                    i\\n                  \\n                \\n                x\\n              \\n              \\n                /\\n              \\n              \\n                D\\n                \\n                  o\\n                  x\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {C_{s}}{C_{g}}}={\\\\frac {1+{k_{i}x}/D_{ox}}{1+k_{i}/h_{g}+{k_{i}x}/D_{ox}}}}\\n  Assuming a diffusion controlled growth i.e. where \\n  \\n    \\n      \\n        \\n          J\\n          \\n            o\\n            x\\n            i\\n            d\\n            e\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{oxide}}\\n   determines the growth rate, and substituting \\n  \\n    \\n      \\n        \\n          C\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          C\\n          \\n            s\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C_{s}}\\n   in terms of \\n  \\n    \\n      \\n        \\n          C\\n          \\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C_{g}}\\n   from the above two relations into \\n  \\n    \\n      \\n        \\n          J\\n          \\n            o\\n            x\\n            i\\n            d\\n            e\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{oxide}}\\n   and \\n  \\n    \\n      \\n        \\n          J\\n          \\n            r\\n            e\\n            a\\n            c\\n            t\\n            i\\n            n\\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{reacting}}\\n   equation respectively, one obtains:\\n\\n  \\n    \\n      \\n        \\n          J\\n          \\n            o\\n            x\\n            i\\n            d\\n            e\\n          \\n        \\n        =\\n        \\n          J\\n          \\n            r\\n            e\\n            a\\n            c\\n            t\\n            i\\n            n\\n            g\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                C\\n                \\n                  g\\n                \\n              \\n            \\n            \\n              1\\n              +\\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                /\\n              \\n              \\n                h\\n                \\n                  g\\n                \\n              \\n              +\\n              \\n                \\n                  k\\n                  \\n                    i\\n                  \\n                \\n                x\\n              \\n              \\n                /\\n              \\n              \\n                D\\n                \\n                  o\\n                  x\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{oxide}=J_{reacting}={\\\\frac {k_{i}C_{g}}{1+k_{i}/h_{g}+{k_{i}x}/D_{ox}}}}\\n  If N is the concentration of the oxidant inside a unit volume of the oxide, then the oxide growth rate can be written in the form of a differential equation. The solution to this equation gives the oxide thickness at any time t.\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              x\\n            \\n            \\n              d\\n              t\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              J\\n              \\n                o\\n                x\\n                i\\n                d\\n                e\\n              \\n            \\n            N\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                C\\n                \\n                  g\\n                \\n              \\n              \\n                /\\n              \\n              N\\n            \\n            \\n              1\\n              +\\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                /\\n              \\n              \\n                h\\n                \\n                  g\\n                \\n              \\n              +\\n              \\n                \\n                  k\\n                  \\n                    i\\n                  \\n                \\n                x\\n              \\n              \\n                /\\n              \\n              \\n                D\\n                \\n                  o\\n                  x\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {dx}{dt}}={\\\\frac {J_{oxide}}{N}}={\\\\frac {k_{i}C_{g}/N}{1+k_{i}/h_{g}+{k_{i}x}/D_{ox}}}}\\n  \\n\\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        A\\n        x\\n        =\\n        B\\n        t\\n        +\\n        \\n          \\n            \\n              x\\n              \\n                i\\n              \\n            \\n          \\n          \\n            2\\n          \\n        \\n        +\\n        A\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{2}+Ax=Bt+{x_{i}}^{2}+Ax_{i}}\\n  \\n\\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        A\\n        x\\n        =\\n        B\\n        (\\n        t\\n        +\\n        τ\\n        )\\n      \\n    \\n    {\\\\displaystyle x^{2}+Ax=B(t+\\\\tau )}\\n  where the constants \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   encapsulate the properties of the reaction and the oxide layer respectively, and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is the initial layer of oxide that was present at the surface. These constants are given as:\\n\\n  \\n    \\n      \\n        A\\n        =\\n        2\\n        \\n          D\\n          \\n            o\\n            x\\n          \\n        \\n        (\\n        \\n          \\n            1\\n            \\n              k\\n              \\n                i\\n              \\n            \\n          \\n        \\n        +\\n        \\n          \\n            1\\n            \\n              h\\n              \\n                g\\n              \\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle A=2D_{ox}({\\\\frac {1}{k_{i}}}+{\\\\frac {1}{h_{g}}})}\\n  \\n\\n  \\n    \\n      \\n        B\\n        =\\n        \\n          \\n            \\n              2\\n              \\n                D\\n                \\n                  o\\n                  x\\n                \\n              \\n              \\n                C\\n                \\n                  s\\n                \\n              \\n            \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B={\\\\frac {2D_{ox}C_{s}}{N}}}\\n  \\n\\n  \\n    \\n      \\n        τ\\n        =\\n        \\n          \\n            \\n              \\n                x\\n                \\n                  i\\n                \\n                \\n                  2\\n                \\n              \\n              +\\n              A\\n              \\n                x\\n                \\n                  i\\n                \\n              \\n            \\n            B\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\tau ={\\\\frac {x_{i}^{2}+Ax_{i}}{B}}}\\n  where \\n  \\n    \\n      \\n        \\n          C\\n          \\n            s\\n          \\n        \\n        =\\n        H\\n        \\n          P\\n          \\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C_{s}=HP_{g}}\\n  , with \\n  \\n    \\n      \\n        H\\n      \\n    \\n    {\\\\displaystyle H}\\n   being the gas solubility parameter of the Henry\\'s law and \\n  \\n    \\n      \\n        \\n          P\\n          \\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P_{g}}\\n   is the partial pressure of the diffusing gas. \\nSolving the quadratic equation for x yields:\\n\\n  \\n    \\n      \\n        x\\n        (\\n        t\\n        )\\n        =\\n        \\n          \\n            \\n              −\\n              A\\n              +\\n              \\n                \\n                  \\n                    \\n                      A\\n                      \\n                        2\\n                      \\n                    \\n                  \\n                  +\\n                  4\\n                  (\\n                  B\\n                  )\\n                  (\\n                  t\\n                  +\\n                  τ\\n                  )\\n                \\n              \\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x(t)={\\\\frac {-A+{\\\\sqrt {{A^{2}}+4(B)(t+\\\\tau )}}}{2}}}\\n  Taking the short and long time limits of the above equation reveals two main modes of operation. The first mode, where the growth is linear, occurs initially when \\n  \\n    \\n      \\n        t\\n        +\\n        τ\\n      \\n    \\n    {\\\\displaystyle t+\\\\tau }\\n   is small. The second mode gives a quadratic growth and occurs when the oxide thickens as the oxidation time increases.\\n\\n  \\n    \\n      \\n        t\\n        +\\n        τ\\n        ≪\\n        \\n          \\n            \\n              A\\n              \\n                2\\n              \\n            \\n            \\n              4\\n              B\\n            \\n          \\n        \\n        ⇒\\n        x\\n        (\\n        t\\n        )\\n        =\\n        \\n          \\n            B\\n            A\\n          \\n        \\n        (\\n        t\\n        +\\n        τ\\n        )\\n      \\n    \\n    {\\\\displaystyle t+\\\\tau \\\\ll {\\\\frac {A^{2}}{4B}}\\\\Rightarrow x(t)={\\\\frac {B}{A}}(t+\\\\tau )}\\n  \\n\\n  \\n    \\n      \\n        t\\n        +\\n        τ\\n        ≫\\n        \\n          \\n            \\n              A\\n              \\n                2\\n              \\n            \\n            \\n              4\\n              B\\n            \\n          \\n        \\n        ⇒\\n        x\\n        (\\n        t\\n        )\\n        =\\n        \\n          \\n            B\\n            (\\n            t\\n            +\\n            τ\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle t+\\\\tau \\\\gg {\\\\frac {A^{2}}{4B}}\\\\Rightarrow x(t)={\\\\sqrt {B(t+\\\\tau )}}}\\n  The quantities B and B/A are often called the quadratic and linear reaction rate constants.  They depend exponentially on temperature, like this:\\n\\n  \\n    \\n      \\n        B\\n        =\\n        \\n          B\\n          \\n            0\\n          \\n        \\n        \\n          e\\n          \\n            −\\n            \\n              E\\n              \\n                A\\n              \\n            \\n            \\n              /\\n            \\n            k\\n            T\\n          \\n        \\n        ;\\n        \\n        B\\n        \\n          /\\n        \\n        A\\n        =\\n        (\\n        B\\n        \\n          /\\n        \\n        A\\n        \\n          )\\n          \\n            0\\n          \\n        \\n        \\n          e\\n          \\n            −\\n            \\n              E\\n              \\n                A\\n              \\n            \\n            \\n              /\\n            \\n            k\\n            T\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B=B_{0}e^{-E_{A}/kT};\\\\quad B/A=(B/A)_{0}e^{-E_{A}/kT}}\\n  where \\n  \\n    \\n      \\n        \\n          E\\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E_{A}}\\n   is the activation energy and \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   is the Boltzmann Constant in eV.  \\n  \\n    \\n      \\n        \\n          E\\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E_{A}}\\n   differs from one equation to the other.  The following table lists the values of the four parameters for single-crystal silicon under conditions typically used in industry (low doping, atmospheric pressure).  The linear rate constant depends on the orientation of the crystal (usually indicated by the Miller indices of the crystal plane facing the surface).  The table gives values for <100> and <111> silicon.\\n\\nValidity for silicon\\nThe Deal–Grove model works very well for single-crystal silicon under most conditions.  However, experimental data shows that very thin oxides (less than about 25 nanometres) grow much more quickly in \\n  \\n    \\n      \\n        \\n          O\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle O_{2}}\\n   than the model predicts. In silicon nanostructures (e.g. Silicon Nanowires) this rapid growth is generally followed by diminishing oxidation kinetics in a process known as self-limiting oxidation, necessitating a modification of the Deal–Grove model.If the oxide grown in a particular oxidation step will significantly exceed 25 nm, a simple adjustment accounts for the aberrant growth rate.  The model yields accurate results for thick oxides if, instead of assuming zero initial thickness (or any initial thickness less than 25 nm), we assume that 25 nm of oxide exists before oxidation begins.  However, for oxides near to or thinner than this threshold, more sophisticated models must be used.\\nIn the 1980s, it became obvious that an update to the Deal-Grove Model would be necessary to model the aforementioned thin oxides (self-limiting cases). One such approach more accurately models thin oxides is the Massoud Model from 1985 [2]. The Massoud Model is analytical and based on parallel oxidation mechanisms. It changes the parameters of Deal-Grove Model to better model the initial oxide growth with the addition of rate enhancement terms. \\nDeal-Grove also fails for polycrystalline silicon (\"poly-silicon\").  First, the random orientation of the crystal grains makes it difficult to choose a value for the linear rate constant.  Second, oxidant molecules diffuse rapidly along grain boundaries, so that poly-silicon oxidizes more rapidly than single-crystal silicon.\\nDopant atoms strain the silicon lattice, and make it easier for silicon atoms to bond with incoming oxygen.  This effect may be neglected in many cases, but heavily doped silicon oxidizes significantly faster.  The pressure of the ambient gas also affects oxidation rate.\\n\\nReferences\\nBibliography\\nMassoud, H. Z.; J.D. Plummer (1985). \"Thermal oxidation of silicon in dry oxygen: Accurate determination of the kinetic rate constants\". Journal of the Electrochemical Society. 132 (11): 2693–2700. doi:10.1149/1.2113649.\\nJaeger, Richard C. (2002). \"Thermal Oxidation of Silicon\". Introduction to Microelectronic Fabrication (2nd ed.). Upper Saddle River: Prentice Hall. ISBN 0-201-44494-1.\\nDeal, B. E.; A. S. Grove (December 1965). \"General Relationship for the Thermal Oxidation of Silicon\". Journal of Applied Physics. 36 (12): 3770–3778. doi:10.1063/1.1713945.\\n\\nExternal links\\nOnline Calculator including pressure, doping, and thin oxide effects',\n",
              " 'The IEEE Engineering in Medicine and Biology Society (EMBS) is an IEEE group dedicated to the study of Biomedical Engineering.\\n\\nHistory\\nThe IRE Professional Group on Medical Electronics was formed in 1952 to consider \"problems in biology and medicine which might be aided in solution by use of electronic engineering principles and devices.\" After IRE\\'s merge with AIEE in 1963, the Professional Group on Medical Electronics merged with AIEE\\'s Committee on Electrical Techniques in Medicine and Biology to form the IEEE Engineering in Medicine and Biology Society.\\n\\nPublications\\nMagazines\\nIEEE PulseIEEE Engineering in Medicine and Biology Magazine (ceased publication in 2010)\\n\\nJournals\\nIEEE Transactions on Biomedical Engineering\\nIEEE Transactions on Neural Systems and Rehabilitation Engineering\\nIEEE Transactions on Information Technology in Biomedicine\\nIEEE Transactions on Medical Imaging,  Editor-in-Chief: Leslie Ying\\nIEEE Transactions on NanoBioscience\\nIEEE/ACM Transactions on Computational Biology and Bioinformatics\\nIEEE Transactions on Biomedical Circuits and Systems\\n\\nConferences\\nThe Annual International Conference of the IEEE Engineering in Medicine and Biology Society is held in various locations in the United States and around the world. \\nThe 40th Annual International Conference was held in Honolulu, Hawaii, USA, on July 17-21, 2018.\\n\\nReferences\\nExternal links\\nEMBS Official Website\\nEMBS Web site\\nEMBC 2008 Web site',\n",
              " \"The 136th Civil Engineer Squadron (136 CES) is a unit of the 136th Airlift Wing, Texas Air National Guard, Texas Military Forces, stationed at Naval Air Station Fort Worth Joint Reserve Base, Fort Worth, Texas. If activated to federal service, the Squadron is gained by the United States Air Force Air Mobility Command.\\n\\nSummary\\nAir Force Civil Engineering is an exciting and dynamic contingency related career field. Prime Base Engineer Emergency Force (Prime BEEF) personnel are called upon to participate in recovery operations as a result of natural and man-made disasters, or maybe subject to deploy and employ forces in hostile environments both conventional and non-conventional warfare.\\nWartime tasked units must be prepared to mobilize and deploy within 52-hours of notification. In support of the Geographic Combatant Commanders and warfighters, the 136th Civil Engineer Squadron provides engineering support for initial Force Beddown at bare base and forward operating areas, base recovery after attack in contingency environments, rapid runway repair, and response to nuclear, biological, chemical, and conventional attacks.\\nThe unit's peacetime role primarily focuses on contingency training in support of our wartime tasking, base infrastructure support, humanitarian aid to underprivileged countries, and assisting the citizens of the State of Texas in times of natural or man-made disasters. Unit expertise is divided into four main functional areas: Command and Control, Engineering, Emergency Management, and Operations, which consists of Electrical Systems, Power Production, Mechanical Systems, Utility/Fuel Systems, Horizontal (Heavy Equipment), and Vertical (Structural) Flights.\\n\\nLineage\\nConstituted as 136th Civil Engineering Flight on 1 October 1969 \\nAssignment to National Guard Bureau on 1 October 1969\\nExtended Federal Recognition on 1 November 1969 \\nAssigned to Texas Air National Guard on 1 November 1969\\nAssigned to 136th Tactical Airlift Wing on 1 November 1969\\nStationed at Naval Air Station Dallas, Hensley Field on 1 November 1969\\nRe-Designated as 136th Civil Engineering Squadron on 1 July 1985\\nAssigned to Air Combat Command on 1 October 1993\\nRe-Designated as 136th Civil Engineer Squadron on 1 March 1994\\nAssigned to Air Mobility Command on 1 April 1997\\nStationed at Naval Air Station Fort Worth Joint Reserve Base on 27 April 1999\\n\\nDecorations\\n Air Force Outstanding Unit Award\\n1972 (1 May 1969 - 30 Apr 1977) (136ARW)\\n1985 (1 Jan 1980 - 1 Aug 1982) (136CEF)\\n1985 (1 Jan 1983 - 31 Dec 1984) (136AW)\\n1991 (1 Sep 1989 - 1 Jun 1991) (136AW)\\n2009 (1 Oct 2006 - 30 Sep 2008) (136CES)\\n2015 (1 Oct 2012 - 30 Sep 2014) (136AW)\\n Texas Governor's Unit Citation\\n\\n\\n=== References ===\",\n",
              " 'In electronics, signal conditioning is the manipulation of an analog signal in such a way that it meets the requirements of the next stage for further processing. \\nIn an analog-to-digital converter application, signal conditioning includes voltage or current limiting and anti-aliasing filtering.\\nIn control engineering applications, it is common to have a sensing stage (which consists of a sensor), a signal conditioning stage (where usually amplification of the signal is done) and a processing stage (often carried out by an ADC and a micro-controller). Operational amplifiers (op-amps) are commonly employed to carry out the amplification of the signal in the signal conditioning stage. In some transducers this feature will come inherent for example in Hall effect sensors.\\nIn power electronics, before processing the input sensed signals by sensors like voltage sensor and current sensor, signal conditioning scales signals to level acceptable to the microprocessor.\\n\\nInputs\\nSignal inputs accepted by signal conditioners include DC voltage and current, AC voltage and current, frequency and electric charge.  Sensor inputs can be accelerometer, thermocouple, thermistor, resistance thermometer, strain gauge or bridge, and LVDT or RVDT.  Specialized inputs include encoder, counter or tachometer, timer or clock, relay or switch, and other specialized inputs.  Outputs for signal conditioning equipment can be voltage, current, frequency, timer or counter, relay, resistance or potentiometer, and other specialized outputs.\\n\\nProcesses\\nSignal conditioning can include amplification, filtering, converting, range matching, isolation and any other processes required to make sensor output suitable for processing after conditioning.\\n\\nFiltering\\nFiltering is the most common signal conditioning function, as usually not all the signal frequency spectrum contains valid data. The common example is 50/60 Hz AC power lines, present in most environments, which cause noise if amplified.\\n\\nAmplification\\nSignal amplification performs two important functions: increases the resolution of the input signal, and increases its signal-to-noise ratio.  For example, the output of an electronic temperature sensor, which is probably in the millivolts range is probably too low for an analog-to-digital converter (ADC) to process directly. In this case it is necessary to bring the voltage level up to that required by the ADC.\\nCommonly used amplifiers used for signal conditioning include sample and hold amplifiers, peak detectors, log amplifiers, antilog amplifiers, instrumentation amplifiers and programmable gain amplifiers.\\n\\nAttenuation\\nAttenuation, the opposite of amplification, is necessary when voltages to be digitized are beyond the ADC range. This form of signal conditioning decreases the input signal amplitude so that the conditioned signal is within ADC range. Attenuation is typically necessary when measuring voltages that are more than 10 V.\\n\\nExcitation\\nExternal power is required for the operation of an active sensor. (E.g. a temperature sensor like a thermistor & RTD, a pressure sensor (piezo-resistive and capacitive), etc.). The stability and precision of the excitation signal directly relates to the sensor accuracy and stability.\\n\\nLinearization\\nLinearization is necessary when sensors produce voltage signals that are not linearly related to the physical measurement. Linearization is the process of interpreting the signal from the sensor and can be done either with signal conditioning or through software.\\n\\nElectrical isolation\\nSignal isolation may be used to pass the signal from the source to the measuring device without a physical connection. It is often used to isolate possible sources of signal perturbations that could otherwise follow the electrical path from the sensor to the processing circuitry. In some situations, it may be important to isolate the potentially expensive equipment used to process the signal after conditioning from the sensor.\\nMagnetic or optical isolation can be used. Magnetic isolation transforms the signal from a voltage to a magnetic field so the signal can be transmitted without physical connection (for example, using a transformer). Optical isolation works by using an electronic signal to modulate a signal encoded by light transmission (optical encoding). The decoded light transmission is then used for input for the next stage of processing.\\n\\nSurge protection\\nA surge protector absorbs voltage spikes to protect the next stage from damage.\\n\\n\\n== References ==',\n",
              " 'Within industry, piping is a system of pipes used to convey fluids (liquids and gases) from one location to another. The engineering discipline of piping design studies the efficient transport of fluid.Industrial process piping (and accompanying in-line components) can be manufactured from wood, fiberglass, glass, steel, aluminum, plastic, copper, and concrete. The in-line components, known as fittings, valves, and other devices, typically sense and control the pressure, flow rate and temperature of the transmitted fluid, and usually are included in the field of piping design (or piping engineering), though the sensors and automatic controlling devices may alternatively be treated as part of instrumentation and control design. Piping systems are documented in piping and instrumentation diagrams (P&IDs).  If necessary, pipes can be cleaned by the tube cleaning process.\\nPiping sometimes refers to piping design, the detailed specification of the physical piping layout within a process plant or commercial building. In earlier days, this was sometimes called drafting, technical drawing, engineering drawing, and design, but is today commonly performed by designers that have learned to use automated computer-aided drawing or computer-aided design (CAD) software.\\nPlumbing is a piping system with which most people are familiar, as it constitutes the form of fluid transportation that is used to provide potable water and fuels to their homes and businesses. Plumbing pipes also remove waste in the form of sewage, and allow venting of sewage gases to the outdoors. Fire sprinkler systems also use piping, and may transport nonpotable or potable water, or other fire-suppression fluids.\\nPiping also has many other industrial applications, which are crucial for moving raw and semi-processed fluids for refining into more useful products. Some of the more exotic materials used in pipe construction are Inconel, titanium, chrome-moly and various other steel alloys.\\n\\nEngineering sub-fields\\nGenerally, industrial piping engineering has three major sub-fields:\\n\\nPiping material\\nPiping design\\nStress analysis\\n\\nStress analysis\\nProcess piping and power piping are typically checked by pipe stress engineers to verify that the routing, nozzle loads, hangers, and supports are properly placed and selected such that allowable pipe stress is not exceeded under different loads such as sustained loads, operating loads, pressure testing loads, etc., as stipulated by the ASME B31, EN 13480, GOST 32388, RD 10-249 or any other applicable codes and standards. It is necessary to evaluate the mechanical behavior of the piping under regular loads (internal pressure and thermal stresses) as well under occasional and intermittent loading cases such as earthquake, high wind or special vibration, and water hammer. This evaluation is usually performed with the assistance of a specialized (finite element) pipe stress analysis computer programs such as AutoPIPE, CAEPIPE,, CAESAR, PASS/START-PROF.In cryogenic pipe supports, most steel become more brittle as the temperature decreases from normal operating conditions, so it is necessary to know the temperature distribution for cryogenic conditions. Steel structures will have areas of high stress that may be caused by sharp corners in the design, or inclusions in the material.\\n\\nMaterials\\nThe material with which a pipe is manufactured often forms as the basis for choosing any pipe. Materials that are used for manufacturing pipes include:\\n\\nCarbon steel\\nASTM A252 Spec Grade 1, Grade 2, Grade 3 Steel Pile Pipe\\nPlastic piping, e.g. HDPE pipe, PP-R pipe or LDPE pipe .\\nLow temperature service carbon steel\\nStainless steel\\nNonferrous metals, e.g. cupro-nickel, tantalum lined, etc.\\nNonmetallic, e.g. tempered glass, Teflon lined, PVC, etc.\\n\\nHistory\\nEarly wooden pipes were constructed out of logs that had a large hole bored lengthwise through the center. Later wooden pipes were constructed with staves and hoops similar to wooden barrel construction. Stave pipes have the advantage that they are easily transported as a compact pile of parts on a wagon and then assembled as a hollow structure at the job site. Wooden pipes were especially popular in mountain regions where transport of heavy iron or concrete pipes would have been difficult.\\nWooden pipes were easier to maintain than metal, because the wood did not expand or contract with temperature changes as much as metal and so consequently expansion joints and bends were not required. The thickness of wood afforded some insulating properties to the pipes which helped prevent freezing as compared to metal pipes. Wood used for water pipes also does not rot very easily. Electrolysis doesn\\'t affect wood pipes at all, since wood is a much better electrical insulator.\\nIn the Western United States where redwood was used for pipe construction, it was found that redwood had \"peculiar properties\" that protected it from weathering, acids, insects, and fungus growths. Redwood pipes stayed smooth and clean indefinitely while iron pipe by comparison would rapidly begin to scale and corrode and could eventually plug itself up with the corrosion.\\n\\nStandards\\nThere are certain standard codes that need to be followed while designing or manufacturing any piping system. Organizations that promulgate piping standards include:\\n\\nASME - The American Society of Mechanical Engineers - B31 series\\nASME B31.1 Power piping (steam piping etc.)\\nASME B31.3 Process piping\\nASME B31.4 Pipeline Transportation Systems for Liquid Hydrocarbons and Other Liquids and oil and gas\\nASME B31.5 Refrigeration piping and heat transfer components\\nASME B31.8 Gas transmission and distribution piping systems\\nASME B31.9 Building services piping\\nASME B31.11 Slurry Transportation Piping Systems (Withdrawn, Superseded by B31.4)\\nASME B31.12 Hydrogen Piping and Pipelines\\nASTM - American Society for Testing and Materials\\nASTM A252 Standard Specification for Welded and Seamless Steel Pipe Piles\\nAPI - American Petroleum Institute\\nAPI 5L Petroleum and natural gas industries—Steel pipe for pipeline transportation systems\\nCWB - Canadian Welding Bureau\\nEN 13480 - European metallic industrial piping code\\nEN 13480-1 Metallic industrial piping - Part 1: General\\nEN 13480-2 Metallic industrial piping - Part 2: Materials\\nEN 13480-3 Metallic industrial piping - Part 3: Design and calculation\\nEN 13480-4 Metallic industrial piping - Part 4: Fabrication and installation\\nEN 13480-5 Metallic industrial piping - Part 5: Inspection and testing\\nEN 13480-6 Metallic industrial piping - Part 6: Additional requirements for buried piping\\nPD TR 13480-7 Metallic industrial piping - Part 7: Guidance on the use of conformity assessment procedures\\nEN 13480-8 Metallic industrial piping - Part 8: Additional requirements for aluminium and aluminium alloy piping\\nGOST, RD, SNiP, SP - Russian piping codes\\nRD 10-249 Power Piping\\nGOST 32388 Process Piping, HDPE Piping\\nSNiP 2.05.06-85 & SP 36.13330.2012 Gas and Oil transmission piping systems\\nEN 1993-4-3 Eurocode 3 — Design of steel structures - Part 4-3: Pipelines\\nAWS - American Welding Society\\nAWWA - American Water Works Association\\nMSS – Manufacturers\\' Standardization Society\\nANSI - American National Standards Institute\\nNFPA - National Fire Protection Association\\nEJMA - Expansion Joint Manufacturers Association\\nIntro to pipe stress  - https://web.archive.org/web/20161008161619/http://oakridgebellows.com/metal-expansion-joints/metal-expansion-joints-in-one-minute/part-1-thermal-growth%26#x20;(one minute)\\n\\nSee also\\nReferences\\nFurther reading\\nASME B31.3 Process Piping Guide, Revision 2 from Los Alamos National Laboratory Engineering Standards Manual OST220-03-01-ESM\\nSeismic Design and Retrofit of Piping Systems, July 2002 from American Lifelines Alliance website\\nEngineering and Design, Liquid Process Piping. Engineer manual, entire document  • (index page) • U.S. Army Corps of Engineers, EM 1110-l-4008, May 1999\\nIntegral Principals of The Structural Dynamics of Flow By L G Claret\\n\\nExternal links\\nBuilding services piping links at Curlie',\n",
              " \"A wet wing is an aerospace engineering technique where an aircraft's wing structure is sealed and used as a fuel tank. Wet wings are also called integral fuel tanks.Wet wings are common among most civilian designs, from large transport aircraft, such as airliners, to small general aviation aircraft. Because the tanks are an integral part of the structure, they cannot be removed, and require access panels for routine maintenance and visual inspections.A disadvantage of the wet wing is that every rivet, bolt, nut plate, hose and tube that penetrates the wing must be sealed to prevent fuel from leaking or seeping around these hardware components. This sealant must allow for expansion and contraction due to rapid temperature changes (such as when cold fuel is pumped into a warm wing tank) and must retain its sealing properties when submerged in fuel and when left dry for long periods of time. Working with this sealant can be difficult and replacing old sealant inside a small wing tank can be harder if the old sealant needs to be removed as well before new sealant can be applied.Notable accidents in which the wet wing design and its drawbacks were causative include Chalk's Ocean Airways Flight 101 and the 1961 Goldsboro B-52 crash.\\n\\n\\n== References ==\",\n",
              " \"Protocol engineering is the application of systematic methods to the development of communication protocols. It uses many of the principles of software engineering, but it is specific to the development of distributed systems.\\n\\nHistory\\nWhen the first experimental and commercial computer networks were developed in the 1970s, the concept of protocols was not yet well developed. These were the first distributed systems. In the context of the newly adopted layered protocol architecture (see OSI model), the definition of the protocol of a specific layer should be such that any entity implementing that specification in one computer would be compatible with any other computer containing an entity implementing the same specification, and their interactions should be such that the desired communication service would be obtained. On the other hand, the protocol specification should be abstract enough to allow different choices for the implementation on different computers. \\nIt was recognized that a precise specification of the expected service provided by the given layer was important . It is important for the verification of the protocol, which should demonstrate that the communication service is provided if both protocol entities implement the protocol specification correctly. This principle was later followed during the standardization of the OSI protocol stack, in particular for the transport layer.\\nIt was also recognized that some kind of formalized protocol specification would be useful for the verification of the protocol and for developing implementations, as well as test cases for checking the conformance of an implementation against the specification . While initially mainly finite-state machine were used as (simplified) models of a protocol entity , in the 1980s three formal specification languages were standardized, two by ISO  and one by ITU . The latter, called SDL, was later used in industry and has been merged with UML state machines.\\n\\nPrinciples\\nThe following are the most important principles for the development of protocols:\\nLayered architecture: A protocol layer at the level n consists of two (or more) entities that have a service interface through which the service of the layer is provided to the users of the protocol, and which uses the service provided by a local entity of level (n-1).\\nThe service specification of a layer describes, in an abstract and global view, the behavior of the layer as visible at the service interfaces of the layer.\\nThe protocol specification defines the requirements that should be satisfied by each entity implementation.\\nProtocol verification consists of showing that two (or more) entities satisfying the protocol specification will provide at their service interfaces the specified service of that layer.\\nThe (verified) protocol specification is used mainly for the following two activities:The development of an entity implementation. Note that the abstract properties of the service interface are defined by the service specification (and also used by the protocol specification), but the detailed nature of the interface can be chosen during the implementation process, separately for each entity.\\nTest suite development for conformance testing. Protocol conformance testing checks that a given entity implementation conforms to the protocol specification. The conformance test cases are developed based on the protocol specification and are applicable to all entity implementations. Therefore standard conformance test suites have been developed for certain protocol standards.\\n\\nMethods and tools\\nTools for the activities of protocol verification, entity implementation and test suite development can be developed when the protocol specification is written in a formalized language which can be understood by the tool. As mentioned, formal specification languages have been proposed for protocol specification, and the first methods and tools where based on finite-state machine models. Reachability analysis was proposed to understand all possible behaviors of a distributed system, which is essential for protocol verification. This was later complemented with model checking. However, finite-state descriptions are not powerful enough to describe constraints between message parameters and the local variables in the entities. Such constraints can be described by the standardized formal specification languages mentioned above, for which powerful tools have been developed. \\nIt is in the field of protocol engineering that model-based development was used very early. These methods and tools have later been used for software engineering as well as hardware design, especially for distributed and real-time systems. On the other hand, many methods and tools developed in the more general context of software engineering can also be used of the development of protocols, for instance model checking for protocol verification, and agile methods for entity implementations.\\n\\nConstructive methods for protocol design\\nMost protocols are designed by human intuition and discussions during the standardization process. However, some methods have been proposed for using constructive methods possibly supported by tools to automatically derive protocols that satisfy certain properties. The following are a few examples:\\n\\nSemi-automatic protocol synthesis: The user defines all message sending actions of the entities, and the tool derives all necessary reception actions (even if several messages are in transit).\\nSynchronizing protocol: The state transitions of one protocol entity are given by the user, and the method derives the behavior of the other entity such that it remains in states that correspond to the former entity.\\nProtocol derived from service specification: The service specification is given by the user and the method derives a suitable protocol for all entities.\\nProtocol for control applications: The specification of one entity (called the plant - which must be controlled) is given, and the method derives a specification of the other entity such that certain fail states of the plant are never reached and certain given properties of the plant's service interactions are satisfied. This is a case of supervisory control.\\n\\nBooks\\nMing T. Liu, Protocol Engineering, Advances in Computers, Volume 29, 1989, Pages 79-195.\\nG.J. Holzmann, Design and Validation of Computer Protocols, Prentice Hall, 1991.\\nH. König, Protocol Engineering, Springer, 2012.\\nM. Popovic, Communication Protocol Engineering, CRC Press, 2nd Ed. 2018.\\nP. Venkataram, S.S. Manvi, B.S. Babu,  Communication Protocol Engineering, 2014.\\n\\n\\n== References ==\",\n",
              " 'Human Factors in Engineering and Design is an engineering textbook, currently in its seventh edition. The book, first published in 1957, is considered a classic in human factors and ergonomics, and one of the best-established texts in the field. It is frequently taught in upper-level and graduate courses in the U.S., and is relied on by practicing human factors and ergonomics professionals.The text is divided into six sections: Introduction; Information Input; Human Output and Control; Work Space and Arrangement; Environment; and Human Factors: Selected Topics.\\n\\nSee also\\nAnthropometry\\nIndustrial and organizational psychology\\n\\n\\n== References ==',\n",
              " 'The tyranny of numbers was a problem faced in the 1960s by computer engineers.  Engineers were unable to increase the performance of their designs due to the huge number of components involved. In theory, every component needed to be wired to every other component (or at least many other components) and were typically strung and soldered by hand. In order to improve performance, more components would be needed, and it seemed that future designs would consist almost entirely of wiring.\\n\\nHistory\\nThe first known recorded use of the term in this context was made by the Vice President of Bell Labs in an article celebrating the 10th anniversary of the invention of the transistor, for the \"Proceedings of the IRE\" (Institute of Radio Engineers), June 1958 [1]. Referring to the problems many designers were having, he wrote:\\n\\nFor some time now, electronic man has known how \\'in principle\\' to extend greatly his visual, tactile, and mental abilities through the digital transmission and processing of all kinds of information. However, all these functions suffer from what has been called \\'the tyranny of numbers.\\' Such systems, because of their complex digital nature, require hundreds, thousands, and sometimes tens of thousands of electron devices.\\nAt the time, computers were typically built up from a series of \"modules\", each module containing the electronics needed to perform a single function. A complex circuit like an adder would generally require several modules working in concert. The modules were typically built on printed circuit boards of a standardized size, with a connector on one edge that allowed them to be plugged into the power and signaling lines of the machine, and were then wired to other modules using twisted pair or coaxial cable.\\nSince each module was relatively custom,  modules were assembled and soldered by hand or with limited automation. As a result, they suffered major reliability problems. Even a single bad component or solder joint could render the entire module inoperative. Even with properly working modules, the mass of wiring connecting them together was another source of construction and reliability problems. As computers grew in complexity, and the number of modules increased, the complexity of making a machine actually work grew more and more difficult. This was the \"tyranny of numbers\".\\nIt was precisely this problem that Jack Kilby was thinking about while working at Texas Instruments. Theorizing that germanium could be used to make all common electronic components - resistors, capacitors, etc. - he set about building a single-slab component that combined the functionality of an entire module. Although successful in this goal, it was Robert Noyce\\'s silicon version and the associated fabrication techniques that make the integrated circuit (IC) truly practical.\\nUnlike modules, ICs were built using photoetching techniques on an assembly line, greatly reducing their cost. Although any given IC might have the same chance of working or not working as a module, they cost so little that if they didn\\'t work you simply threw it away and tried another. In fact, early IC assembly lines had failure rates around 90% or greater, which kept their prices high. The U.S. Air Force and NASA were major purchasers of early ICs, where their small size and light weight overcame any cost issues. They demanded high reliability, and the industry\\'s response not only provided the desired reliability but meant that the increased yield had the effect of driving down prices.\\nICs from the early 1960s were not complex enough for general computer use, but as the complexity increased through the 1960s, practically all computers switched to IC-based designs. The result was what are today referred to as the third-generation computers, which became commonplace during the early 1970s. The progeny of the integrated circuit, the microprocessor, eventually superseded the use of individual ICs as well, placing the entire collection of modules onto one chip.\\nSeymour Cray was particularly well known for making complex designs work in spite of the tyranny of numbers. His attention to detail and ability to fund several attempts at a working design meant that pure engineering effort could overcome the problems they faced. Yet even Cray eventually succumbed to the problem during the CDC 8600 project, which eventually led to him leaving Control Data.\\n\\nReferences\\n\"The Chip that Jack Built\", Texas Instruments']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1mejStGylS7",
        "colab_type": "text"
      },
      "source": [
        "### arXiv dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeRgQUxxlc1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "71153cce-6b82-4737-ec49-5b53a3910861"
      },
      "source": [
        "#Accounting for stochasticity of the method\n",
        "best_hparams = {'preprocess': 'custom', 'dm': 0, 'epochs': 50, 'vector_size': 50, 'min_count': 1, 'window': 3}\n",
        "top2_acc_list = list()\n",
        "top1_acc_list = list()\n",
        "\n",
        "for i in range(30):\n",
        "    x_arxiv_papers = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "    y_arxiv_labels = list()\n",
        "    for category in arxiv_dataset:\n",
        "        n_papers = len(category['papers'])\n",
        "        category_class_id = category['label']\n",
        "        id = random.randint(0,n_articles-1)\n",
        "        #print(\"Paper #{} was chosen from topic {}\".format(id, category_class_id))\n",
        "        x_arxiv_papers[category_class_id] = \". \".join([category['papers'][id]['title'], category['papers'][id]['abstract']])\n",
        "        y_arxiv_labels.append(category_class_id)\n",
        "\n",
        "\n",
        "    max_sim_model = MaxSimClassifier(\"wiki\", best_hparams['preprocess'], best_hparams['vector_size'],best_hparams['min_count'],\n",
        "                                    best_hparams['epochs'],best_hparams['dm'],best_hparams['window'],workers=8)\n",
        "\n",
        "    max_sim_model.fit_articles(x_arxiv_papers, y_arxiv_labels)\n",
        "\n",
        "    top2_acc = max_sim_model.score(x_test_CLF_A_full, y_test_CLF_A_full,eval=\"top2\") \n",
        "    top1_acc = max_sim_model.score(x_test_CLF_A_full, y_test_CLF_A_full,eval=\"top1\")\n",
        "\n",
        "    print(\"Run {}. MSC Top-1 Acc: {:.4f}, Top-2 Acc.:{:.4f}\".format(i, top1_acc, top2_acc))\n",
        "    top2_acc_list.append(top2_acc)\n",
        "    top1_acc_list.append(top1_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run 0. MSC Top-1 Acc: 0.1375, Top-2 Acc.:0.2687\n",
            "Run 1. MSC Top-1 Acc: 0.2112, Top-2 Acc.:0.3837\n",
            "Run 2. MSC Top-1 Acc: 0.2100, Top-2 Acc.:0.3663\n",
            "Run 3. MSC Top-1 Acc: 0.1837, Top-2 Acc.:0.3100\n",
            "Run 4. MSC Top-1 Acc: 0.1837, Top-2 Acc.:0.3337\n",
            "Run 5. MSC Top-1 Acc: 0.2000, Top-2 Acc.:0.3137\n",
            "Run 6. MSC Top-1 Acc: 0.2050, Top-2 Acc.:0.3038\n",
            "Run 7. MSC Top-1 Acc: 0.1837, Top-2 Acc.:0.3475\n",
            "Run 8. MSC Top-1 Acc: 0.1625, Top-2 Acc.:0.3025\n",
            "Run 9. MSC Top-1 Acc: 0.2062, Top-2 Acc.:0.3162\n",
            "Run 10. MSC Top-1 Acc: 0.1950, Top-2 Acc.:0.3375\n",
            "Run 11. MSC Top-1 Acc: 0.1650, Top-2 Acc.:0.3088\n",
            "Run 12. MSC Top-1 Acc: 0.1888, Top-2 Acc.:0.2963\n",
            "Run 13. MSC Top-1 Acc: 0.1713, Top-2 Acc.:0.3150\n",
            "Run 14. MSC Top-1 Acc: 0.2362, Top-2 Acc.:0.3713\n",
            "Run 15. MSC Top-1 Acc: 0.1600, Top-2 Acc.:0.3575\n",
            "Run 16. MSC Top-1 Acc: 0.1475, Top-2 Acc.:0.2462\n",
            "Run 17. MSC Top-1 Acc: 0.1975, Top-2 Acc.:0.3125\n",
            "Run 18. MSC Top-1 Acc: 0.2288, Top-2 Acc.:0.3675\n",
            "Run 19. MSC Top-1 Acc: 0.2100, Top-2 Acc.:0.3625\n",
            "Run 20. MSC Top-1 Acc: 0.2425, Top-2 Acc.:0.3713\n",
            "Run 21. MSC Top-1 Acc: 0.2250, Top-2 Acc.:0.3475\n",
            "Run 22. MSC Top-1 Acc: 0.2062, Top-2 Acc.:0.3275\n",
            "Run 23. MSC Top-1 Acc: 0.2263, Top-2 Acc.:0.3875\n",
            "Run 24. MSC Top-1 Acc: 0.2075, Top-2 Acc.:0.3638\n",
            "Run 25. MSC Top-1 Acc: 0.2225, Top-2 Acc.:0.3500\n",
            "Run 26. MSC Top-1 Acc: 0.2013, Top-2 Acc.:0.3025\n",
            "Run 27. MSC Top-1 Acc: 0.1688, Top-2 Acc.:0.3025\n",
            "Run 28. MSC Top-1 Acc: 0.1900, Top-2 Acc.:0.3325\n",
            "Run 29. MSC Top-1 Acc: 0.1963, Top-2 Acc.:0.3225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvG6YgFx0mnT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "384aa8e4-b2f0-4c8a-81bc-e1f540255b4f"
      },
      "source": [
        "print(\"Avg Top-2 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top2_acc_list)),np.std(np.array(top2_acc_list))))\n",
        "print(\"Avg Top-1 acc:{:.4f}+-{:.4f}\".format(np.mean(np.array(top1_acc_list)),np.std(np.array(top1_acc_list))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg Top-2 acc:0.3310+-0.0332\n",
            "Avg Top-1 acc:0.1957+-0.0255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vD6Es7u0OLz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "6f48f1b5-4513-4bba-d574-08ba955a0b9e"
      },
      "source": [
        "x_arxiv_papers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Mechanism to Mitigate AVX-Induced Frequency Reduction. Modern Intel CPUs reduce their frequency when executing wide vector operations (AVX2 and AVX-512 instructions), as these instructions increase power consumption. The frequency is only increased again two milliseconds after the last code section containing such instructions has been executed in order to prevent excessive numbers of frequency changes. Due to this delay, intermittent use of wide vector operations can slow down the rest of the system significantly. For example, previous work has shown the performance of web servers to be reduced by up to 10% if the SSL library uses AVX-512 vector instructions. These performance variations are hard to predict during software development as the performance impact of vectorization depends on the specific workload. We describe a mechanism to reduce the slowdown caused by wide vector instructions without requiring extensive changes to existing software. Our design allows the developer to mark problematic AVX code regions. The scheduler then restricts execution of this code to a subset of the cores so that only these cores' frequency is affected. Threads are automatically migrated to a suitable core whenever necessary. We identify a suitable load balancing policy to ensure good utilization of all available cores. Our approach is able to reduce the performance variability caused by AVX2 and AVX-512 instructions by over 70%. △ Less\",\n",
              " 'New dynamics of energy use and CO2 emissions in China. Global achievement of climate change mitigation will heavy reply on how much of CO2 emission has and will be released by China. After rapid growth of emissions during last decades, China CO2 emissions declined since 2014 that driven by decreased coal consumption, suggesting a possible peak of China coal consumption and CO2 emissions. Here, by combining a updated methodology and underlying data from different sources, we reported the soaring 5.5% (range: +2.5% to +8.5% for one sigma) increase of China CO2 emissions in 2018 compared to 2017, suggesting China CO2 is not yet to peak and leaving a big uncertain to whether China emission will continue to rise in the future. Although our best estimate of total emission (9.9Gt CO2 in 2018) is lower than international agencies in the same year, the results show robust on a record-high energy consumption and total CO2 emission in 2018. During 2014-2016, China energy intensity (energy consumption per unit of GDP) and total CO2 emissions has decreased driven by energy and economic structure optimization. However, the decrease in emissions is now offset by stimulates of heavy industry production under economic downturn that driving coal consumption (+5% in 2018), as well as the surging of natural gas consumption (+18% in 2018) due to the government led coal-to-gas energy transition to reduce local air pollutions. Timing policy and actions are urgent needed to address on these new drivers to turn down the total emission growth trend. △ Less',\n",
              " 'A balanced energy consumption clustering algorithm for heterogeneous energy wireless sensor networks. In this paper, a balanced energy consumption clustering algorithm (BECC) is proposed. This new scheme is a cluster-based algorithm designed for heterogeneous energy wireless sensor networks. A polarized energy factor is introduced to adjust the probability with which each node may become a cluster head in the election of the new clustering scheme. Under the condition that the expected number of cluster heads in the network preserves the theoretical optimal number, BECC makes sure that nodes with higher residual energy will become cluster heads with higher probabilities while nodes with lower residual energy will not become cluster heads. Simulation results show that this new scheme provides longer lifetime than the classical clustering algorithms including LEACH and other improved algorithms in heterogeneous networks, and BECC also reaches larger amount of messages received at the sink. △ Less',\n",
              " 'A geometric characterisation of sensitivity analysis in monomial models. Sensitivity analysis in probabilistic discrete graphical models is usually conducted by varying one probability value at a time and observing how this affects output probabilities of interest. When one probability is varied then others are proportionally covaried to respect the sum-to-one condition of probability laws. The choice of proportional covariation is justified by a variety of optimality conditions, under which the original and the varied distributions are as close as possible under different measures of closeness. For variations of more than one parameter at a time proportional covariation is justified in some special cases only. In this work, for the large class of discrete statistical models entertaining a regular monomial parametrisation, we demonstrate the optimality of newly defined proportional multi-way schemes with respect to an optimality criterion based on the notion of I-divergence. We demonstrate that there are varying parameters choices for which proportional covariation is not optimal and identify the sub-family of model distributions where the distance between the original distribution and the one where probabilities are covaried proportionally is minimum. This is shown by adopting a new formal, geometric characterization of sensitivity analysis in monomial models, which include a wide array of probabilistic graphical models. We also demonstrate the optimality of proportional covariation for multi-way analyses in Naive Bayes classifiers. △ Less',\n",
              " 'Reply to arXiv:1810.11506: Size Effect According to the Critical Shear Crack Theory (CSCT) for Reinforced Concrete Beams and Slabs. This document is a reply to arXiv:1810.11506. In that document, Dönmez and Bažant raise a number of criticism on the hypotheses Critical Shear Crack Theory (CSCT). The aspects criticized have however been largely discussed in previous works of the CSCT, proving the validity of the theory. This document compiles such previous knowledge as a reply to the authors of the report. △ Less',\n",
              " 'Effect of cryopreservation on the structural and functional integrity of cell membranes of sugarcane (Saccharum sp.) embryogenic calluses. In this paper, we investigated if the differences consistently noted in survival and plantlet production between cryopreserved and non-cryopreserved, control sugarcane embryogenic calluses were related to modifications induced during cryopreservation in the structural and functional integrity of cell membranes. For this, the evolution of electrolyte leakage, lipid peroxidation products and cell membrane protein contents was measured during 5 d after cryopreservation. Differences between control and frozen calluses were observed only during the first 2 (electrolyte leakage) or 3 d (lipid peroxidation products and membrane protein content) after freezing. It was not possible to link these differences with the differences noted in survival and plant production between control and cryopreserved calluses. Additional studies are thus needed to elucidate which biochemical factors, linked to survival and plantlet regeneration, are affected by cryopreservation. △ Less',\n",
              " 'Portfolio Rebalancing under Uncertainty Using Meta-heuristic Algorithm. In this paper, we solve portfolio rebalancing problem when security returns are represented by uncertain variables considering transaction costs. The performance of the proposed model is studied using constant-proportion portfolio insurance (CPPI) as rebalancing strategy. Numerical results showed that uncertain parameters and different belief degrees will produce different efficient frontiers, and affect the performance of the proposed model. Moreover, CPPI strategy performs as an insurance mechanism and limits downside risk in bear markets while it allows potential benefit in bull markets. Finally, using a globally optimization solver and genetic algorithm (GA) for solving the model, we concluded that the problem size is an important factor in solving portfolio rebalancing problem with uncertain parameters and to gain better results, it is recommended to use a meta-heuristic algorithm rather than a global solver. △ Less',\n",
              " 'A Geometric Theory of Higher-Order Automatic Differentiation. First-order automatic differentiation is a ubiquitous tool across statistics, machine learning, and computer science. Higher-order implementations of automatic differentiation, however, have yet to realize the same utility. In this paper I derive a comprehensive, differential geometric treatment of automatic differentiation that naturally identifies the higher-order differential operators amenable to automatic differentiation as well as explicit procedures that provide a scaffolding for high-performance implementations. △ Less']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}